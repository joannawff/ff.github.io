<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>Category: Technology - This is Feifei&#39;s Blog.</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="">



<meta name="keywords" content="NLP,ML,Data Science">



    <meta property="og:type" content="website">
<meta property="og:title" content="This is Feifei&#39;s Blog.">
<meta property="og:url" content="http://example.com/categories/Technology/index.html">
<meta property="og:site_name" content="This is Feifei&#39;s Blog.">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Feifei">
<meta property="article:tag" content="NLP,ML,Data Science">
<meta name="twitter:card" content="summary">






<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0"></head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                    
                    Feifei
                    
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/categories/LifeStyle">Lifestyle</a>
            
            <a class="navbar-item "
               href="/categories/Music">Music</a>
            
            <a class="navbar-item is-active"
               href="/categories/Technology">Technology</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5><i class="far fa-folder"></i>Technology</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2021/12/03/data-augmentation/" itemprop="url">Data Augmentation (DA)</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2021-12-03T13:56:32.000Z" itemprop="datePublished">Dec 3 2021</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            a minute read (About 168 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2021/12/03/data-augmentation/data_augmentation.jpeg"></p>
<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p>Three DA methods: Paraphrasing, Noising, Sampling.<br><img src="/2021/12/03/data-augmentation/DA_for_nlp.jpeg"></p>
<h2 id="Paraphrasing"><a href="#Paraphrasing" class="headerlink" title="Paraphrasing"></a>Paraphrasing</h2><p>Six methods: Thesaurus, Semantic Embeddings, MLMs, Rules, Machine Translation, Model Generation.<br><img src="/2021/12/03/data-augmentation/paraphrasing.jpeg"></p>
<h2 id="Noising"><a href="#Noising" class="headerlink" title="Noising"></a>Noising</h2><p>Five methods: Swapping, Deletion, Insertion, Substitution, Mixup.<br>Others in ConSERT: Dropout, Feature Cut-off.<br><img src="/2021/12/03/data-augmentation/noising.jpeg"></p>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>Four methods: Rules, Seq2Seq Models, Language Models, Self-training.<br><img src="/2021/12/03/data-augmentation/sampling.jpeg"></p>
<h2 id="How-to-select-DA-methods"><a href="#How-to-select-DA-methods" class="headerlink" title="How to select DA methods"></a>How to select DA methods</h2><p>Comparison in six dimensions.<br>Level: t=text, e=embedding, l=label<br>Granularity: w=word, p=phrase, s=sentence.<br><img src="/2021/12/03/data-augmentation/dimensions.jpeg"></p>
<h2 id="Tools-for-DA"><a href="#Tools-for-DA" class="headerlink" title="Tools for DA"></a>Tools for DA</h2><ul>
<li>Easy DA: <a target="_blank" rel="noopener" href="https://github.com/jasonwei20/eda_nlp">https://github.com/jasonwei20/eda_nlp</a></li>
<li>Unsupervised DA：<a target="_blank" rel="noopener" href="https://github.com/google-research/uda">https://github.com/google-research/uda</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/makcedward/nlpaug">https://github.com/makcedward/nlpaug</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhanlaoban/eda_nlp_for_Chinese">https://github.com/zhanlaoban/eda_nlp_for_Chinese</a></li>
</ul>
<h2 id="Optimization-for-DA-data"><a href="#Optimization-for-DA-data" class="headerlink" title="Optimization for DA data"></a>Optimization for DA data</h2><p>Two methods: pre-train &amp; parameter sweeping.</p>
<ul>
<li>pre-train<br>Pre-train with augmented data while fine-tune with labeled data.</li>
<li>parameter sweeping<br><img src="/2021/12/03/data-augmentation/parameter_sweeping.jpeg"></li>
</ul>
<h2 id="DA-Application"><a href="#DA-Application" class="headerlink" title="DA Application"></a>DA Application</h2><p><img src="/2021/12/03/data-augmentation/application.jpeg"></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420295576">https://zhuanlan.zhihu.com/p/420295576</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.01852">Data Augmentation Approaches in Natural Language Processing: A Survey</a></li>
</ul>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/01/27/tools/" itemprop="url">常用工具安装</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-01-27T08:16:01.000Z" itemprop="datePublished">Jan 27 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            2 minutes read (About 267 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2018/01/27/tools/tools.jpeg"></p>
<h1 id="office2016安装与激活"><a href="#office2016安装与激活" class="headerlink" title="office2016安装与激活"></a>office2016安装与激活</h1><ol>
<li>下载office2016,解压，安装</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1htsmyxQ">https://pan.baidu.com/s/1htsmyxQ</a></p>
<ol start="2">
<li>下载激活工具kms，解压，安装</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1kXafD1X">https://pan.baidu.com/s/1kXafD1X</a></p>
<ol start="3">
<li>打开kms，点击红色按钮，激活成功自动关闭，打开office，即可使用。</li>
</ol>
<h1 id="SecureCRT安装与激活"><a href="#SecureCRT安装与激活" class="headerlink" title="SecureCRT安装与激活"></a>SecureCRT安装与激活</h1><h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><ol>
<li>下载SecureCRT（含注册机）</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1nwRUdjr">https://pan.baidu.com/s/1nwRUdjr</a></p>
<ol start="2">
<li><p>点击scrt8.13_x64.exe，安装完成后不要打开，使其处于未使用状态</p>
</li>
<li><p>将/8.x.x注册机/keygen.exe拷贝至步骤2中SecureCRT的安装目录下，运行keygen.exe</p>
</li>
<li><p>点击patch，中间会要求选择SecureCRT.exe和LicenceHelper.exe的位置，由于已经放在了SecureCRT安装目录下，会自动显示对应的文件，选择即可。</p>
</li>
<li><p>打开SecureCRT8.1, 点输入注册码-&gt;人工输入，将keygen里的name、company、date、licence number填入相应的地方即可，feature留白不用填。</p>
</li>
</ol>
<p>至此，安装与激活结束。</p>
<h2 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h2><p>和Windows类似，下载安装即可。<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1rasmvzI">https://pan.baidu.com/s/1rasmvzI</a></p>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/01/20/ubuntu-install-anaconda/" itemprop="url">anaconda——unbuntu16.04下安装</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2018-01-20T08:00:51.000Z" itemprop="datePublished">Jan 20 2018</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            a minute read (About 190 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2018/01/20/ubuntu-install-anaconda/anaconda.jpeg"></p>
<ol>
<li>在官网下载Anaconda，注意版本</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.anaconda.com/download/#linux">https://www.anaconda.com/download/#linux</a></p>
<ol start="2">
<li>在终端输入如下命令<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd 下载</span><br><span class="line">bash Anaconda2-5.0.1-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure></li>
<li>一直按回车键，中间有两次要填写yes的时候</li>
</ol>
<p>第一次：同意接受协议</p>
<p>第二次：同意将Anaconda2安装路径写入/home/joanna/.bashrc</p>
<ol start="4">
<li>至此，安装成功，最后出现提示</li>
</ol>
<p>Thank you for installing Anaconda2!</p>
<ol start="5">
<li>验证，在终端输入python，发现还是系统默认的pyhton，没有Anaconda等字样出现，则在shell输入命令<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></tbody></table></figure>
重新输入python，系统显示<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19)</span><br></pre></td></tr></tbody></table></figure>
在shell输入命令<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></tbody></table></figure>
系统显示一大堆可用的packages，至此验证结束。</li>
</ol>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/woainishifu/article/details/74978647">http://blog.csdn.net/woainishifu/article/details/74978647</a></li>
</ul>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2017/04/15/stanford-nlp-lecture1/" itemprop="url">课程学习——斯坦福CS224N深度学习自然语言处理lecture1</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2017-04-15T05:47:52.000Z" itemprop="datePublished">Apr 15 2017</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            6 minutes read (About 890 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2017/04/15/stanford-nlp-lecture1/lecture1.png"></p>
<p>最近斯坦福大学新推出了一门课《Natural Language Processing with Deep Learning》，这对于NLP领域的初学者来说是一门很好的教学课程。本人作为nlp小白，写此文主要作为个人的学习笔记，有什么不当之处还请读者指点。<br>lecture1以入门介绍为主，讲述nlp、dl的定义、历史发展和基本方法。</p>
<h1 id="NLP-Natural-Language-Processing"><a href="#NLP-Natural-Language-Processing" class="headerlink" title="NLP(Natural Language Processing)"></a>NLP(Natural Language Processing)</h1><p>NLP是一门CS（计算机科学）、AI（人工智能）、Linguistic（语言学）交叉的学科，研究的是如何让计算机理解人类的自然语言的一门学科。若输入的是语音，则先进行语音分析（phonetic analysis）；若输入是文本，则先进行OCR文字识别或切词处理（Tokenization），然后进行如下四个层次的分析；</p>
<ul>
<li>形态学分析（morphological analysis）</li>
<li>语法分析（syntacic analysis）</li>
<li>语义分析（semantic analysis）</li>
<li>语篇处理（discourse processing）</li>
</ul>
<h1 id="Human-Language"><a href="#Human-Language" class="headerlink" title="Human Language"></a>Human Language</h1><p>如果把语言比作信号，那么人类语言就像是一个精确的信号系统，可以通过声音、动作和图像来进行编码，大脑通过连续不断地的激活以获得最准确的意思。此外，人类本身大脑存储了海量的知识库，表达的语言具有上下文强依赖关系。</p>
<h1 id="DL-Deep-Learning"><a href="#DL-Deep-Learning" class="headerlink" title="DL(Deep Learning)"></a>DL(Deep Learning)</h1><p>DL属于ML（Machine Learning）的一个分支，解决的是机器真正自动学习的问题。在80~00年代，ML任务并非真正的机器学习任务，需要人为地设计表征形式和特征，而这个过程也是最重要、工作量最大的部分，整个过程需要人去艰难地学习理论知识、设计各种特征、分析预测结果……久而久之ML任务就变成了一个参数优化的体力活。</p>
<p>DL有个比较重要的概念——表征学习（Representation Learning），这是一个自动学习的过程，能够从原始的信号中自动学习到好的特征，总结出优秀、重要的表征形式。近年来随着GPU、分布式等存储计算能力的提高、数据集的扩大，DL在语音识别和图像处理两个领域的突破尤为显著。<br>DL和ML的对比可以从图中看出：</p>
<h1 id="Deep-NLP"><a href="#Deep-NLP" class="headerlink" title="Deep NLP"></a>Deep NLP</h1><p>从近来的研究论文中也能发现，深度学习开始在NLP领域占领重要的作用。不像原来的手工提取特征计算的方式，Deep NLP有个重要的概念——向量空间（vector spaces），首先将文字转化成向量，然后向量结合成新的向量，用激活函数得到新的表征。<br>例如：</p>
<ul>
<li><p>传统的NLP形态学分析，单词由前缀（prefix）+词干（stem）+后缀（suffix）组成，而DL将其转化成了vector，两两带权重累加形成新的向量，如图所示：</p>
</li>
<li><p>机器翻译是NLP研究的开端，SMT（Statistical Machine Learning）是NLP发展史一个的里程碑，而如今的机器翻译大多以NMT（Nerual Machine Learning）为主，将源语句子映射为一个向量后再处理输出目标语句子。</p>
</li>
</ul>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2017/04/08/stanford-nlp-lecture/" itemprop="url">课程学习——斯坦福CS224N深度学习自然语言处理lecture2</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2017-04-08T07:35:14.000Z" itemprop="datePublished">Apr 8 2017</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            9 minutes read (About 1351 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2017/04/08/stanford-nlp-lecture/lecture.jpeg"></p>
<p>课程第二讲主要介绍向量表达、word2vec原理以及skip-gram求上下文概率方程的推导过程。</p>
<h1 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h1><p>本节视频里简单提了一下wordnet，但为了加深理解，我认为有必要对wordnet进行深层剖析。WordNet是一种基于认知语言学的、具有上下位关系的英语语义词典，根据词义组成了一个覆盖范围宽广的词汇语义网。</p>
<p>WordNet里有两个重要的概念，一个是同义词集，另一个是同义词之间的关系。它们分别用两个CSV文件来保存：synsets.txt和hypernym.txt。</p>
<h2 id="同义词集"><a href="#同义词集" class="headerlink" title="同义词集"></a>同义词集</h2><p>synsets.txt保存了一系列的同义词，每行由3列，每列用逗号分隔，形如：id,synset,gloss<br>其中；id=编号，synset=同义词集（单词间用空格分隔），gloss=注解<br>举例：</p>
<p>45,AND_circuit AND_gate,a circuit in a computer that fires only when all of its inputs fire</p>
<h2 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h2><p>hypernyms.txt保存同义词集之间的关系，每行是由逗号分隔的id序列，形如：id1,id2,id3…</p>
<ul>
<li>继承关系（hyperonymy, hyponymy, is-a）</li>
</ul>
<p>这类描述的是通用词与具体词之间的关系，比如：<br>通用词：{furniture, piece_of_furniture}<br>具体词：{bed} {bunkbed}</p>
<ul>
<li>部分-整体关系(part-whole)</li>
</ul>
<p>这类描述的是部分与整体之间的包含关系，比如整体={seat}，部分={leg}</p>
<ul>
<li>交叉词性关系（Cross-POS）</li>
</ul>
<p>实际上，WordNet主要按照词性（POS）来获取词间关系，因此WordNet含有4个子网络，分别存储名词、动词、形容词和副词。Cross-POS连接的是词干相同、词性不同的词，如ovserve(verb)、observant(adjective)、observation(noun)。</p>
<h2 id="SAP（shortest-ancestral-path，最短祖先路径）"><a href="#SAP（shortest-ancestral-path，最短祖先路径）" class="headerlink" title="SAP（shortest ancestral path，最短祖先路径）"></a>SAP（shortest ancestral path，最短祖先路径）</h2><p>SAP是WordNet的一种数据结构，表示两个不同结点到共同祖先的最短距离，这可以用来计算两个词之间的距离相似度。如图所示，结点3和结点11的SAP=4：</p>
<p>现有两个名词A和B，则A与B之间的距离distance(A,B)计算方式为：<br>step1: 若A、B都不是名词，distance=∞<br>step2: distance = min(sap(v(A),w(B))),其中V(A)表示A的任意同义词集，w(B)表示B的任意同义词集。</p>
<p>##Outcast<br>Outcast就是找出在一组词中与其他最不相似的词，计算方法相对SAP比较简单暴力，就是计算某词语其他词的距离之和，最大的那个就是要求的解了。即：</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(di)^2 = (dist(Ai, A1))^2 + (dist(Ai, A2))^2 + … + (dist(Ai, An))^2</span><br></pre></td></tr></tbody></table></figure>
<p>注意，wordnet工具提供了distance(),sap()和outcast()方法。</p>
<h2 id="NLTKL与WordNet"><a href="#NLTKL与WordNet" class="headerlink" title="NLTKL与WordNet"></a>NLTKL与WordNet</h2><p>NLTK也有WordNet接口，详情参考《NLTK源码阅读——WordNet》</p>
<h1 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h1><p>尽管WordNet能够一定程度上表达单词得的含义，但是具有如下不足：</p>
<ul>
<li>新词的时效性跟不上</li>
<li>需要人工去搜集、创建</li>
<li>不考虑上下文信息</li>
<li>难以准确地定位两个词的相似度</li>
</ul>
<p>word2vec是google于2013年提出的工具，通过词向量（word embedding）来更好地度量词与词之间的相似性。起初vector的做法one-hot向量法：对于一个句子，如”I am a student.”，每个单词的向量表示为：</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">I: [1,0,0,0]</span><br><span class="line">am: [0,1,0,0]</span><br><span class="line">a: [0,0,1,0]</span><br><span class="line">student: [0,0,0,1]</span><br></pre></td></tr></tbody></table></figure>
<p>即单词的向量维度K=词典大小，这很容易造成维度灾难问题。对于这种情况，对应的解决办法是利用向量空间模型（Vector Space Model，简称VSM）将one-hot向量映射为稠密连续的Distributed Representation，其基本思想是：通过训练将每一个词映射成一个固定长度的短向量，构成词向量空间，每一个词向量相当于是空间中的一个点，那么点间距离就可以用来度量词间相似度了。</p>
<p>Distributed Representation的形式如[0.792,-0.177,-0.107,0.109,-0.542,…]，维度一般是50-100维，向量表示不唯一。</p>
<p>word2vec主要包含Skip-gram和Continious Bag of Words(CBOW)两种模型，分别采用Hierarchical Softmax 和Negative Sampling两种框架来实现。Ski-gram指给定中心词求上下文的概率，CBOW则相反，指给定上下文求中心词的概率。下面以Hierachical Softmax实现CBOW模型为例介绍实现的过程。</p>
<h2 id="CBOW模型的Hierachical-Softmax实现"><a href="#CBOW模型的Hierachical-Softmax实现" class="headerlink" title="CBOW模型的Hierachical Softmax实现"></a>CBOW模型的Hierachical Softmax实现</h2><p>模型有3层：输入层、投影层、输出层. 注意：word embedding是术语表达，而word2vec是google提出的工具，并非指模型或算法。<br>在word2vec中采用Huffman编码，利用词频作为Huffman树的结点权值，权值大的编码为1，为左孩子结点；权值小的编码为0，为右孩子结点。</p>
<h2 id="SG（Skip-Gram）"><a href="#SG（Skip-Gram）" class="headerlink" title="SG（Skip-Gram）"></a>SG（Skip-Gram）</h2><p>SG根据中心词预测上下文，包含三层：input（输入层）、projection（投影层）、output（输出层）</p>
<h2 id="CBOW（Continious-Bag-of-Words）"><a href="#CBOW（Continious-Bag-of-Words）" class="headerlink" title="CBOW（Continious Bag of Words）"></a>CBOW（Continious Bag of Words）</h2><p>CBOW正好与SG相反，根据上下文预测中心词。</p>
<ul>
<li>Hierachical Softmax</li>
<li>Negative Sample</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li>wordnet</li>
<li>《Word2Vec的前世今生》</li>
<li>《Distributed Representations ofWords and Phrases and their Compositionality》</li>
</ul>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2016/12/10/nltk-collocation/" itemprop="url">nltk源码-collocation</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2016-12-10T07:24:27.000Z" itemprop="datePublished">Dec 10 2016</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 minutes read (About 1703 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2016/12/10/nltk-collocation/collection.jpeg"></p>
<p>实习期接触了人机多轮对话领域，师傅的工作是多轮理解，我的任务是用特征工程在小样本上调研用户多轮对话是否意图保持。涉及的技术不深，大部分都是师傅想出策略，然后我去执行，查看效果验证该策略是否可用。</p>
<p>这是一个特征调研的工作，除了NLP的basic seg、pos、ner等特征外，其他的特征都是基于知识库的，也用到了tag（标签）与collocation（搭配）。之前阅读过nltk的collocation源码，它采用的是基于窗口size的搭配策略，与我实际工作中的搭配过程大相径庭。因此，现在花点时间来总结一下nltk和实习期间所接触的collocation。</p>
<h1 id="nltk搭配实验"><a href="#nltk搭配实验" class="headerlink" title="nltk搭配实验"></a>nltk搭配实验</h1><p>NLTK的搭配方法主要采用基于窗口大小的算法，NLTK共提供了二元，三元，四元的搭配抽取，NLTK的搭配过程:</p>
<h2 id="语料处理"><a href="#语料处理" class="headerlink" title="语料处理"></a>语料处理</h2><p>NLTK里的语料采用webtext，内涵6个文件：overheard.txt，pirates.txt，singles.txt，wine.txt，这些文本文件都是txt格式，每句一行，如图：</p>
<p>首先对这些语料进行处理，大小写转换，并将纯文本转化成单词的list格式，返回[];</p>
<h2 id="寻找搭配对"><a href="#寻找搭配对" class="headerlink" title="寻找搭配对"></a>寻找搭配对</h2><p>指定窗口n（默认为2），若抽取非连续短语，则窗口大小要大于抽取的元组大小。如抽取二元组，则window_size&gt;=3。在此过程中，首先创建nltk.probability.FreqDist对象wfd和bfd，获得每个单词在语料库中的频率和元组的联合频率，并指定开头和结尾的字符(默认为None)。此时，所有的n元组都是候选搭配。查找n-gram过程又分为连续和非连续，连续过程由ngram实现，非连续过程由skipgrams实现，找到该句中的所有元组。</p>
<p>以二元组为例：创建BigramCollocatinFinder对象cf，调用BigramCollocatinFinder.from_words函数，利用nltk.util.ngrams得到每句话所有的n元组，并计算得到语料库中的单词频率wfd和元组频率bfd；返回一个新的初始化了wfd和bfd值的BigramCollocatinFinder对象cf。</p>
<h2 id="过滤"><a href="#过滤" class="headerlink" title="过滤"></a>过滤</h2><p>首先过滤单词，如大小写转换，过滤停用词和小于长度为3的单词；可调用apply_freq_filter(min_freq)函数，指定最小频率min_freq，将频率小于min_freq的单词移除，</p>
<p>然后过滤元组，依旧可以调用apply_freq_filter(self, min_freq)函数，指定最小频率min_freq，将频率小于min_freq的候选搭配移除；或是通过apply_ngram_filter(self, fn)，指定函数fn，可过滤一切满足条件fn的元组；</p>
<p>此时，返回一个预处理后的BigramCollocatinFinder或TrigramCollocatinFinder对象cf；</p>
<h2 id="给候选搭配打分，度量搭配关联度（-）"><a href="#给候选搭配打分，度量搭配关联度（-）" class="headerlink" title="给候选搭配打分，度量搭配关联度（*）"></a>给候选搭配打分，度量搭配关联度（*）</h2><p>指定一个打分策略score_fn，生成一个由大到小排序的(ngram, score)序列，由score_ngrams(sef, score_fn)实现；这里的score_fn一般由用户在运行参数中确定，NLTK默认使用似然比（likelihood ratio）来度量元组中单词的关联度，由nltk.metrics.BigramAssocMeasures.likelihood_ratio实现；并将ngram排序，得到(ngram, rank)序列1；</p>
<p>然后根据打分的结果，确定相近的词组找到最佳的Nbest个搭配，即为我们得到的带间隔的搭配短语；</p>
<p>采用新的打分策略compare_fn，NLTK默认采用ngram频率/语料库规模N，可得到新(ngram, rank)序列2；</p>
<p>对这两个序列，采用speraman相关性分析，得到该语料的总体相关程度。</p>
<h2 id="搭配关联分析"><a href="#搭配关联分析" class="headerlink" title="搭配关联分析"></a>搭配关联分析</h2><p>在寻找搭配的过程中涉及到了搭配关联分析，当用户未指定时，NLTK默认采用似然比作为score_fn来分析搭配关系。</p>
<p>初始状态时，score_fn的参数为空；经前期1-3步处理，可得到如下三个结果作为score_fn的参数：料库规模，即语料所有词的词频；每个候选ngram的规模，采用公式：</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">元组出现频率/(窗口大小-1)；</span><br></pre></td></tr></tbody></table></figure>
<p>Ngram中各个词的规模，即词频；通过似然比等把上述三个参数代入计算，得到似然值作为当ngram的得分，以(ngram, score)作为结果降序排列；似然比公式：<br>对于每个(ngram, score)元组，设置一个rank_gap，只有当两个ngram的分数差&gt;rank_gap时才会将ngram列入排序列表中，否则就舍弃，这么做可以增大结果集中各个ngram的得分差，便于speraman分析，这步得到关于ngram排列的元组(ngram, rank)序列1.</p>
<p>除了制定似然比作为score_fn，得到经排序后的(ngram, score)序列，NLTK还要求指定一个compare_score(即ngram频率/语料库大小N)作为打分策略，得到新的(ngram, score)序列2；</p>
<p>进行speraman相关性分析。将上步的两个序列作为参数，传入spearman_correlation中。对于序列1和序列2，计算相同ngram的rank差，并计算speraman相关系数，采用公式：(其中，k表示在序列1和序列2中都出现的ngram），n为k的个数)</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 - (6 res / (n (n*n - 1)))</span><br><span class="line">res = ∑(ranks1[k] - ranks2[k])^2</span><br></pre></td></tr></tbody></table></figure>
<p>因此，计算得到的speraman相关系数在[-1, 1]之间，这个结果就表示了该语料的总体相关程度。NLTK度量搭配候选匹配程度的策略还很多，比如PMI值、Poisson-Stirling法、皮尔逊卡方检验等。</p>
<h2 id="我的实验总结"><a href="#我的实验总结" class="headerlink" title="我的实验总结"></a>我的实验总结</h2><p>我对nltk的搭配策略进行了小小的实验，发现NLTK中带间隔的短语抽取，与连续短语抽取的不同之处在于，当窗口大小大于抽取的元组时，使用skip-gram抽取候选gram。此次仅以单间隔的二元短语为例进行试验，效果不尽如意。从结果中看来，许多连续的短语也都抽取出来，而且非间隔的短语构成搭配的强度不大。分析看来，试验缺乏深层的分析，如过滤连续可搭配的短语等。而且本次实验只采用似然比作为得分策略，不具有比较性。</p>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2016/11/20/reading-knowledge-graph/" itemprop="url">读书笔记——《大叔据智能》第2章：知识图谱</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2016-11-20T07:06:15.000Z" itemprop="datePublished">Nov 20 2016</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            3 minutes read (About 406 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/2016/11/20/reading-knowledge-graph/knowledge_graph.png"></p>
<h1 id="读书笔记——《大叔据智能》第2章：知识图谱"><a href="#读书笔记——《大叔据智能》第2章：知识图谱" class="headerlink" title="读书笔记——《大叔据智能》第2章：知识图谱"></a>读书笔记——《大叔据智能》第2章：知识图谱</h1><p>知识图谱的基本组成单元是实体（entity）；<br>数据来源：</p>
<ul>
<li>大规模知识库：Wikipedia、百度百科、Wordnet，实体关系、领域知识以结构化方式存储；</li>
<li>链接数据：采用RDF框架描述结构化知识，实体关系以三元组方式存在，RDF链接构成语义Web知识库；</li>
<li>网页文本数据：从无结构化网页中抽取结构化信息；</li>
<li>多数据源知识融合：实体、关系与实例融合；</li>
</ul>
<p>主要应用：<br>QU、QA、DR（Document Representation）</p>
<p>采用技术：</p>
<ul>
<li>Entity Linking：实体链指，包含实体识别（EntityRecognition）和实体消歧（EntityDisambiguation）两个任务，涉及文本图像、社交媒体等领域；</li>
<li>Relation Extraction：关系抽取，有bootstrapping、短语抽取、实体对关系分类等方法；<br>实体对关系分类方法中，可以用DistantSupervision启发式标注训练语料；</li>
<li>Knowledge Reasoning：知识推理，涉及PathRanking Algorithnm、PredicateLogical、Markov LogicNetWork等方法，要求依赖规则，可采用关联挖掘技术自动发现推理规则；</li>
<li>Knowledge Representation：知识表示，低维向量表示，采用DistributedRepresentation方案和TransE模型；<br>TransE模型：分布式向量表示，实体关系转化成三元组，实体head-&gt;实体tail的过程中，不断调整h、r、t以达到h+r=t的目的。</li>
</ul>
</body></html>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2016/11/02/linux-commands/" itemprop="url">Linux 命令</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2016-11-02T15:33:11.000Z" itemprop="datePublished">Nov 2 2016</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Technology/">Technology</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            8 minutes read (About 1253 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><img src="/linux.png"></p>
<h1 id="删除行尾-M"><a href="#删除行尾-M" class="headerlink" title="删除行尾^M"></a>删除行尾^M</h1><p>vim下输入:%s/\r//g</p>
<p>注：出现^M一般出现在本地文件上传至linux后，本地文件如excel的换行符默认为\r\n，而linux上的文件是\n，因此会出现这个错误，直接替换即可。</p>
<h1 id="替换字符串"><a href="#替换字符串" class="headerlink" title="替换字符串"></a>替换字符串</h1><p>例：file.txt文件，cat替换成dog</p>
<p>文件内 :g/cat/s//dog/g</p>
<p>文件外 sed -i “s/cat/dog/g” file.txt</p>
<p>修改文件换行符 sed -i “s/\r/\n/g” file.txt</p>
<h1 id="合并文件"><a href="#合并文件" class="headerlink" title="合并文件"></a>合并文件</h1><h2 id="每行合并"><a href="#每行合并" class="headerlink" title="每行合并"></a>每行合并</h2><p>前提：文件行数相同<br>方法一：$ paste -d “\t” file_1 file_2 file_3 … &gt; new_file</p>
<p>方法二：$ awk ‘NR==FNR{a[NR]=$0;nr=NR;}NR&gt;FNR{print a[NR-nr]”\t”$0}’ file_1 file_2</p>
<p>方法三：$ awk ‘{getline f2 &lt; “file_2”; print $0″\t”f2}’ file_1</p>
<h2 id="前后合并"><a href="#前后合并" class="headerlink" title="前后合并"></a>前后合并</h2><p>cat file1 file2 &gt; file3</p>
<h1 id="显示文件某几行"><a href="#显示文件某几行" class="headerlink" title="显示文件某几行"></a>显示文件某几行</h1><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><p>cat filename | tail -n +3000 | head -n 1000</p>
<p>cat filename| head -n 3000 | tail -n +1000</p>
<p>分解：<br>tail -n 1000：显示末尾1000行<br>tail -n +1000：显示1000行以后的所有行<br>head -n 1000：显示开头1000行</p>
<h2 id="seg"><a href="#seg" class="headerlink" title="seg"></a>seg</h2><p>sed -n ‘5,10p’ filename：显示5~10行</p>
<h1 id="查找进程"><a href="#查找进程" class="headerlink" title="查找进程"></a>查找进程</h1><p>ps aux | grep “python” | awk -F ‘ ‘ ‘{print $2}’<br>输出的即为正在运行python进程的PID</p>
<h1 id="统计文件个数"><a href="#统计文件个数" class="headerlink" title="统计文件个数"></a>统计文件个数</h1><p>统计当期那文件夹下文件个数，包括子文件夹</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lR | grep “^-“ | wc -l</span><br></pre></td></tr></tbody></table></figure>
<p>统计文件夹下目录个数，包括子文件夹</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lR | grep “^d” | wc -l</span><br></pre></td></tr></tbody></table></figure>
<p>统计当前文件夹下文件个数</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l | grep “^-“ | wc -l</span><br></pre></td></tr></tbody></table></figure>
<p>统计当前文件夹下目录个数</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l | grep “^d” | wc -l</span><br></pre></td></tr></tbody></table></figure>

<h1 id="中文乱码处理"><a href="#中文乱码处理" class="headerlink" title="中文乱码处理"></a>中文乱码处理</h1><p>SecureCRT连接远程服务器后，发现有中文乱码问题，下面是解决方方案。</p>
<p>查看文件编码,打开文件，输入</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:set fileencoding</span><br></pre></td></tr></tbody></table></figure>
<p>结果显示fileencoding=cp936</p>
<p>修改文件编码，输入</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:set fileencoding=utf-8</span><br></pre></td></tr></tbody></table></figure>
<p>再次输入1中的语句检查文件编码，显示fileencoding=utf-8</p>
<p>编辑vimrc，在终端输入</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.vimrc</span><br></pre></td></tr></tbody></table></figure>
<p>在最后一行添加</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set encoding=utf-8 fileencodings=ucs-bom,utf-8,cp936</span><br></pre></td></tr></tbody></table></figure>

<h1 id="进程后台运行"><a href="#进程后台运行" class="headerlink" title="进程后台运行"></a>进程后台运行</h1><p>进程时间太久，可以通过ctrl+z让当前进程转至后台运行，此时，系统提示</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1]+ Stopped 程序名</span><br></pre></td></tr></tbody></table></figure>
<p>把进程转到后台运行</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bg 1</span><br></pre></td></tr></tbody></table></figure>
<p>查看进程，在终端输入jobs，系统提示</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1]+ Running 程序名</span><br></pre></td></tr></tbody></table></figure>

<h1 id="下载大文件"><a href="#下载大文件" class="headerlink" title="下载大文件"></a>下载大文件</h1><p>在linux上下载uget和aria2</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install uget</span><br><span class="line">sudo apt-get install aria2</span><br></pre></td></tr></tbody></table></figure>
<p>分享百度云盘文件，点击下载，暂停，查看下载地址，如：<br><a target="_blank" rel="noopener" href="https://www.baidupcs.com/rest/2.0/pcs/file?method=batchdownload&amp;app_id=250528&amp;zipcontent=%7B%22fs_id%22:%5B%22206844088160893%22%5D%7D&amp;sign=DCb740ccc5511e5e8fedcff06b081203:4uhuulJJkDD/ovWIkyeJonYrjFw=&amp;uid=3844112066&amp;time=1516433452&amp;dp-logid=432463554070908057&amp;dp-callid=0&amp;vuk=3039518624&amp;from_uk=3844112066">https://www.baidupcs.com/rest/2.0/pcs/file?method=batchdownload&amp;app_id=250528&amp;zipcontent=%7B%22fs_id%22%3A%5B%22206844088160893%22%5D%7D&amp;sign=DCb740ccc5511e5e8fedcff06b081203:4uhuulJJkDD%2FovWIkyeJonYrjFw%3D&amp;uid=3844112066&amp;time=1516433452&amp;dp-logid=432463554070908057&amp;dp-callid=0&amp;vuk=3039518624&amp;from_uk=3844112066</a></p>
<p>在linux上输入命令</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aria2c –dir=保存的目录 –max-connection-per-server=16 –max-concurrent-downloads=16 –split=16 –continue=true 下载连接地址</span><br></pre></td></tr></tbody></table></figure>
<p>然后shell上就显示下载进度，如</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[#e55ab 12MiB/3.1Gib(%) CN:1 DL:61Kib ETA:16h48m39s]</span><br></pre></td></tr></tbody></table></figure>
<p>至此，等待结束即可。</p>
<h1 id="vim高亮查找"><a href="#vim高亮查找" class="headerlink" title="vim高亮查找"></a>vim高亮查找</h1><p>在vim下输入/xxx，显示第一个匹配结果，回车，高亮显示所有匹配结果，按n向下查找</p>
<p>在vim下输入?xxx，显示最后一个匹配结果，回车，高亮所有匹配结果，按n向上查找</p>
<p>当回车并没有高亮所有匹配结果时，输入</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:set hls</span><br></pre></td></tr></tbody></table></figure>
<p>不需要高亮所有时可关闭，输入</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:set nohls</span><br></pre></td></tr></tbody></table></figure>

<h1 id="添加用户并赋予root权限"><a href="#添加用户并赋予root权限" class="headerlink" title="添加用户并赋予root权限"></a>添加用户并赋予root权限</h1><ol>
<li>在shell用root账户输入如下命令<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -d /home/test -s /bin/bash -m test</span><br></pre></td></tr></tbody></table></figure>
其中，</li>
</ol>
<p>-d表示会在主目录生成test用户文件夹，并在该文件夹下生成环境文件（.bash_logout, .bashrc, .cache/, .distlib/, examples.desktop, .pip/, .profile）<br>-s 表示shell 文件，一般都是/bin/bash<br>-m 用户名</p>
<ol start="2">
<li><p>给test用户添加密码，这里要求输入一次密码和一次验证密码</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd test</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>退出，用test用户登录，验证当前帐号：</p>
</li>
</ol>
<ul>
<li>查看 /etc/passwd，每个帐号一行，每行共7列：用户名，密码（x），uid，gid，用户信息说明，主文件夹，shell</li>
<li>查看 /etc/shadow, 用户密码（已加密）</li>
<li>查看/home/test,用ls -a查看.bashrc等隐藏文件是否存在</li>
</ul>
<p>至此，添加用户与root权限完成。</p>
<h1 id="gcc编译显示大量warning-amp-tensorflow安装失败的解决"><a href="#gcc编译显示大量warning-amp-tensorflow安装失败的解决" class="headerlink" title="gcc编译显示大量warning &amp; tensorflow安装失败的解决"></a>gcc编译显示大量warning &amp; tensorflow安装失败的解决</h1><p>现象：在重新添加帐号后，安装librosa时，gcc编译过程中会出现大量的warning。并且，tensorflow始终安装失败，显示</p>
<figure class="highlight plain hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Downloading/unpacking tensorflow</span><br><span class="line">Could not find any downloads that satisfy the requirement tensorflow</span><br><span class="line">Cleaning up…</span><br><span class="line">No distributions at all found fo tensorflow</span><br><span class="line">Storing debug log for failure in /home/xxx/.pip/pip.log</span><br></pre></td></tr></tbody></table></figure>

<p>原因：pip版本较低<br>解决：升级pip，在shell输入命令 pip install –upgrade pip</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="https://wangchujiang.com/linux-command/">linux-command</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zeze/p/6839230.html">https://www.cnblogs.com/zeze/p/6839230.html</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.chinaunix.net/uid-20732478-id-763411.html">http://blog.chinaunix.net/uid-20732478-id-763411.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kex1n/p/5199109.html">https://www.cnblogs.com/kex1n/p/5199109.html</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/lhw413/article/details/72806308">http://blog.csdn.net/lhw413/article/details/72806308</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/41875915/install-tensorflow-on-ubuntu-14-04">https://stackoverflow.com/questions/41875915/install-tensorflow-on-ubuntu-14-04</a></li>
</ul>
</body></html>
    
    </div>
    
    
</article>




    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2021 Feifei&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>