{"pages":[],"posts":[{"title":"My Love","text":"What if I die before you? Continue my life. (Silence) Disappointed? Yeah. I need give our parents the end of their care. (Silence) I would try to die before you. I’m afraid nobody would take care of our parents then. (Silence) If you’ve got cancer, let’s resign and go traverling around the world with all of our belongings. OK.","link":"/2021/05/22/2021-05-22/"},{"title":"BERT and its family","text":"Reference: https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/BERT-and-its-family/"},{"title":"Roberta","text":"","link":"/2021/05/19/Roberta/"},{"title":"Word2Vec vs BPE vs WordPiece","text":"Reference: https://wmathor.com/index.php/archives/1517/","link":"/2021/05/19/BPE-vs-WordPiece/"},{"title":"Open Corpora","text":"https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words","link":"/2021/05/25/Open-Corpora/"},{"title":"data-quality.md","text":"","link":"/2021/03/11/data-quality-md/"},{"title":"Data quality or model quality?","text":"[https://www.talend.com/resources/machine-learning-data-quality/](https://www.talend.com/resources/machine-learning-data-quality/","link":"/2021/03/11/data-quality/"},{"title":"Get Start with Hexo","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/02/27/hello-world/"},{"title":"Multilingual Pretrained Models","text":"","link":"/2021/02/27/multilingual-pretrained-models/"},{"title":"language detect","text":"resource: https://pypi.org/project/langdetect/","link":"/2021/05/25/language-detect/"},{"title":"photo","text":"","link":"/2021/03/11/photo/"},{"title":"wide&deep bert","text":"How to integrate traditional models like GBDT/LR to popular transformer based pretrain-finetune models like BERT/Roberta? Please Reference: Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm GBDT and BERT: a Hybrid Solution for Recognizing Citation Intent Wide And Deep Transformers Applied to Semantic Relatedness and Textual Entailment","link":"/2021/11/02/wide-deep-bert/"},{"title":"Lean about Huggingface Transformers","text":"","link":"/2021/02/27/transformers/"},{"title":"Model Compression","text":"Methods: Network Pruning Knowledge Distillation Parameter Quantization Architecture Design Reference: http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html https://www.bilibili.com/video/BV1yy4y1B7ny/ https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/Model-Compression/"},{"title":"Linux 命令","text":"删除行尾^Mvim下输入:%s/\\r//g 注：出现^M一般出现在本地文件上传至linux后，本地文件如excel的换行符默认为\\r\\n，而linux上的文件是\\n，因此会出现这个错误，直接替换即可。 替换字符串例：file.txt文件，cat替换成dog 文件内 :g/cat/s//dog/g 文件外 sed -i “s/cat/dog/g” file.txt 修改文件换行符 sed -i “s/\\r/\\n/g” file.txt 合并文件每行合并前提：文件行数相同方法一：$ paste -d “\\t” file_1 file_2 file_3 … &gt; new_file 方法二：$ awk ‘NR==FNR{a[NR]=$0;nr=NR;}NR&gt;FNR{print a[NR-nr]”\\t”$0}’ file_1 file_2 方法三：$ awk ‘{getline f2 &lt; “file_2”; print $0″\\t”f2}’ file_1 前后合并cat file1 file2 &gt; file3 显示文件某几行cat30003999行：cat filename | tail -n +3000 | head -n 100010003000行：cat filename| head -n 3000 | tail -n +1000分解： tail -n 1000：显示末尾1000行 tail -n +1000：显示1000行以后的所有行 head -n 1000：显示开头1000行 segsed -n ‘5,10p’ filename：显示5~10行 Reference linux-command","link":"/2016/11/02/linux-commands/"},{"title":"What does NLTK do?","text":"Language processing tasks and corresponding NLTK modules with examples of functionality Language processing task NLTK modules Functionality Accessing corpora corpus standardized interfaces to corpora and lexicons String processing tokenize, stem tokenizers, sentence tokenizers, stemmers Collocation discovery collocations t-test, chi-squared, point-wise mutual information Part-of-speech tagging tag n-gram, backoff, Brill, HMM, TnT Machine learning classify, cluster, tbl decision tree, maximum entropy, naive Bayes, EM, k-means Chunking chunk regular expression, n-gram, named-entity Parsing parse, ccg chart, feature-based, unification, probabilistic, dependency Semantic interpretation sem, inference lambda calculus, first-order logic, model checking Evaluation metrics metrics precision, recall, agreement coefficients Probability and estimation probability frequency distributions, smoothed probability distributions Applications app, chat graphical concordancer, parsers, WordNet browser, chatbots Linguistic fieldwork toolbox manipulate data in SIL Toolbox format Referencehttps://www.nltk.org/book/ch00.html#tab-goals","link":"/2021/11/03/nltk-overview/"},{"title":"How to Onboard NLTK?","text":"Suggested course plans; approximate number of lectures per chapter Chapter Arts and Humanities Science and Engineering 1 Language Processing and Python 2-4 2 2 Accessing Text Corpora and Lexical Resources 2-4 2 3 Processing Raw Text 2-4 2 4 Writing Structured Programs 2-4 1-2 5 Categorizing and Tagging Words 2-4 2-4 6 Learning to Classify Text 0-2 2-4 7 Extracting Information from Text 2 2-4 8 Analyzing Sentence Structure 2-4 2-4 9 Building Feature Based Grammars 2-4 1-4 10 Analyzing the Meaning of Sentences 1-2 1-4 11 Managing Linguistic Data 1-2 1-4 Total 18-36 18-36 Referencehttps://www.nltk.org/book/ch00.html#tab-goals","link":"/2021/11/03/how-to-onboard-NLTK/"},{"title":"Epoch vs. Step","text":"Question: 1000 steps with 1 epoch vs running 100 steps with 10 epoch?Epoch: Number epoch equal to the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, one epoch has completed.Step: process one batch of data.Relation: An epoch consists of one full cycle through the training data. This are usually many steps. As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of 2,000 images / (10 images / step) = 200 steps. More: batch size, iteration, online learningIteration: Every time you pass a batch of data through the neural network, you completed one iteration. In the case of neural networks, that means the forward pass and backward pass.Relation: batch size * number of iterations = epoch Iteration: one batch of training samples.Online learning: batch_size=1 Batch size impact to modelToo small: it might get too slow because of significantly lower computational speed because of not exploiting vectorization to the full extent.Too large: out of memory. Reference: https://androidkt.com/batch-size-step-iteration-epoch-neural-network/ https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow#:~:text=In%20easy%20words.%20Epoch%3A%20Epoch%20is%20considered%20as,and%20batch_size%20%3D%201000%20steps%20%3D%20100.%20Share.","link":"/2021/11/11/epoch-vs-step/"},{"title":"measure data quality","text":"","link":"/2021/11/18/measure-data-quality/"},{"title":"Data Augmentation (DA)","text":"Data AugmentationThree DA methods: Paraphrasing, Noising, Sampling. ParaphrasingSix methods: Thesaurus, Semantic Embeddings, MLMs, Rules, Machine Translation, Model Generation. NoisingFive methods: Swapping, Deletion, Insertion, Substitution, Mixup.Others in ConSERT: Dropout, Feature Cut-off. SamplingFour methods: Rules, Seq2Seq Models, Language Models, Self-training. How to select DA methodsComparison in six dimensions.Level: t=text, e=embedding, l=labelGranularity: w=word, p=phrase, s=sentence. Tools for DA Easy DA: https://github.com/jasonwei20/eda_nlp Unsupervised DA：https://github.com/google-research/uda https://github.com/makcedward/nlpaug https://github.com/zhanlaoban/eda_nlp_for_Chinese Optimization for DA dataTwo methods: pre-train &amp; parameter sweeping. pre-trainPre-train with augmented data while fine-tune with labeled data. parameter sweeping DA Application Reference https://zhuanlan.zhihu.com/p/420295576 Data Augmentation Approaches in Natural Language Processing: A Survey","link":"/2021/12/03/data-augmentation/"},{"title":"linux commands","text":"","link":"/2021/12/04/linux-commands-1/"},{"title":"坝上草原游记","text":"坝上草原游记这是我来帝都的第二个国庆节，这次的节日显得有点孤单，可能是因为我已经一个人出来租房子了吧。前23年的我一直都是和家人、同学一起度过，突然间变成孤身一个人了还真是有点不适应。幸好在偌大的帝都还有表姐，索性就和表姐约上，去北京周边走一走，看一看繁华边上的“京北第一草原”。 早上7:00在中服大厦门口坐上大巴，到目的地的时候已经是14:00左右了，离原定的到达时间晚了2h。不过一路上的风景还是不错的，平日里随处可见高楼大厦，偶尔来到偏远的地方，看看连绵的群山和崎岖的小路，阴暗的天气还夹杂着些小雨，倒别有一番风味了。 一下车便直奔坝上的酒店，路上奔波了大约7h，桌上的菜也凉了，看着没什么食欲，最美味的当数馒头了，幸好自己带了咸菜，就着馒头吃还不错。 由于天气原因，原定当天下午的骑马、越野项目改到了第二天上午，而第二天的大汗行宫之行也提前了。来之前就查了天气，这里昼夜温差大，又正逢秋季风大，所以大衣和披肩是来此旅游必需的装备。在酒店里换好衣服，披上披肩 ，我们就乘着大巴驶向大汗行宫。大汗行宫距酒店大约30分钟的车程，属于自费项目，原价90¥/人，满10人可团购85¥/人。本着下一次来也不知是猴年马月的想法，我们还是买了票，领略一下蒙古民族的“马背文化”。 大汗行宫占地面积很广，一进门就是崇天门，由三座门组成，庄严肃穆。 门后是沙场，放眼望去全是马匹与骑兵，气势恢宏。蒙古部落居住的房子叫蒙古包，正中间的应该是正殿，下面由大车轮拉着，我打趣说这是古代的房车啊。 !蒙古包呈圆形，顶部由圆环组成，像皇帝出行时轿上的华盖。褐色的顶，金色的壁，红色的毯，整体呈现一种庄严的对称美。殿内是一派上朝的景象，中间高坐的成吉思汗，两边正襟危坐的大臣，好似在紧张地商议某个战场。 往里走是鹰帐，帐前是一个庞大的鹰头，双目凌厉，目视前方，嘴唇紧闭，好似在思索，随时准备释放两边的爪子，发起攻击。 大汗行宫就像是个博物馆，蒙古包里再现了部分生活情景、记载了真实的历史事实，如史官商议、波斯美女跳舞，也有陈列历史文物、普及历史知识策，如战马的由来。逛了约1h，冷风吹得头疼，披肩此时就排上用场了，裹在头上刚好能抵挡些阴冷的风。 16:50集合，依旧乘坐大巴去闪电湖，这里的天黑的早，再加上昼夜温差大，部分人选择在车里等，不去闪电湖，而去了的每个人在回来时都不忘对天气埋怨一下。 晚餐有3种选择：自助BBQ、烤全羊、酒店晚餐，每个人的团费不一样，包含的游乐项目与晚餐种类也就不同，而我们的就包含了自助BBQ。自助BBQ和烤全羊在同一个地方，离酒店不远，由铁线网围着，只留一个进出口。拿着导游给的餐票去小屋里领取食材，然后拿火炉去取碳，一个简易的自助BBQ就开始了。 场所是露天的，铁线网外就是一个篝火活动广场，中间一群人围着篝火嗨皮，而广场前则是一个舞台，不时会有个主持人唱歌或调节氛围，也会有游客自告奋勇点歌上去唱。大家都很开心，旁边的老爷子一边烧烤一边还跳起了舞，感觉逃离了城市的喧嚣后，大家都放开了身心，不再拘谨。 第一天的行程很快在歌声中结束了，令我惊喜的是即便没有空调，酒店房间里也不冷，而且还有热水是关键。第二天才是重头戏，是我向往已久的草原行，为了养精蓄锐，我们草草地洗了个澡，回味了会儿白天的图片后，没多久就满意地入睡了。","link":"/2016/11/02/travel-in-bashangcaoyuan/"},{"title":"读书笔记——《大叔据智能》第2章：知识图谱","text":"读书笔记——《大叔据智能》第2章：知识图谱知识图谱的基本组成单元是实体（entity）；数据来源： 大规模知识库：Wikipedia、百度百科、Wordnet，实体关系、领域知识以结构化方式存储； 链接数据：采用RDF框架描述结构化知识，实体关系以三元组方式存在，RDF链接构成语义Web知识库； 网页文本数据：从无结构化网页中抽取结构化信息； 多数据源知识融合：实体、关系与实例融合； 主要应用：QU、QA、DR（Document Representation） 采用技术： Entity Linking：实体链指，包含实体识别（EntityRecognition）和实体消歧（EntityDisambiguation）两个任务，涉及文本图像、社交媒体等领域； Relation Extraction：关系抽取，有bootstrapping、短语抽取、实体对关系分类等方法；实体对关系分类方法中，可以用DistantSupervision启发式标注训练语料； Knowledge Reasoning：知识推理，涉及PathRanking Algorithnm、PredicateLogical、Markov LogicNetWork等方法，要求依赖规则，可采用关联挖掘技术自动发现推理规则； Knowledge Representation：知识表示，低维向量表示，采用DistributedRepresentation方案和TransE模型；TransE模型：分布式向量表示，实体关系转化成三元组，实体head-&gt;实体tail的过程中，不断调整h、r、t以达到h+r=t的目的。","link":"/2016/11/20/reading-knowledge-graph/"}],"tags":[{"name":"diary","slug":"diary","link":"/tags/diary/"},{"name":"NLP,Transformer","slug":"NLP-Transformer","link":"/tags/NLP-Transformer/"},{"name":"NLP, ML, DL, data analysis","slug":"NLP-ML-DL-data-analysis","link":"/tags/NLP-ML-DL-data-analysis/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"travel","slug":"travel","link":"/tags/travel/"},{"name":"knowledge graph","slug":"knowledge-graph","link":"/tags/knowledge-graph/"}],"categories":[{"name":"Technology","slug":"Technology","link":"/categories/Technology/"},{"name":"LifeStyle","slug":"LifeStyle","link":"/categories/LifeStyle/"}]}