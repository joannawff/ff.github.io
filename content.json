{"pages":[],"posts":[{"title":"My Love","text":"What if I die before you? Continue my life. (Silence) Disappointed? Yeah. I need give our parents the end of their care. (Silence) I would try to die before you. I’m afraid nobody would take care of our parents then. (Silence) If you’ve got cancer, let’s resign and go traverling around the world with all of our belongings. OK.","link":"/2021/05/22/2021-05-22/"},{"title":"BERT and its family","text":"Reference: https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/BERT-and-its-family/"},{"title":"Roberta","text":"","link":"/2021/05/19/Roberta/"},{"title":"Word2Vec vs BPE vs WordPiece","text":"Reference: https://wmathor.com/index.php/archives/1517/","link":"/2021/05/19/BPE-vs-WordPiece/"},{"title":"Open Corpora","text":"https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words","link":"/2021/05/25/Open-Corpora/"},{"title":"Data Quality (1)","text":"Note: image is downloaded from here. OverviewBetter Data!=More Data Better data is always better. Improve data quality is alwayss well invested. Tip: doing some form of smart sampling on your population the right way (e.g. using stratified sampling) can get you to better results than if you used the whole unfiltered data set. Data Without a Sound Approach = Noise The availability of data enables more and better insights and applications. More data indeed enables and requires better approaches Attributes related to data quality Accuracy Consistency Timeless Completeness Ways to improve data quality Resolve missing values12df[\"column\"].isnull()df[\"column\"].fillna() Convert the Date feature column to DateTime format1df[\"Date\"]=pd.to_datetime(df[\"Date\"],format=\"%m/%d/%Y\") Parse date/time features123df[\"year\"]=df[\"Date\"].dt.yeardf[\"month\"]=df[\"Date\"].dt.monthdf[\"day\"]=df[\"Date\"].dt.day Remove unwanted values(characters) Convert categorical columns to “one-hot encodings” Data quality process菜市口 Reference https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html https://www.coursera.org/lecture/launching-machine-learning/improve-data-quality-WWXvO https://www.talend.com/resources/machine-learning-data-quality/","link":"/2021/03/11/data-quality/"},{"title":"Get Start with Hexo","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/02/27/hello-world/"},{"title":"Multilingual Pretrained Models","text":"","link":"/2021/02/27/multilingual-pretrained-models/"},{"title":"language detect","text":"resource: https://pypi.org/project/langdetect/","link":"/2021/05/25/language-detect/"},{"title":"photo","text":"","link":"/2021/03/11/photo/"},{"title":"wide&deep bert","text":"How to integrate traditional models like GBDT/LR to popular transformer based pretrain-finetune models like BERT/Roberta? Please Reference: Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm GBDT and BERT: a Hybrid Solution for Recognizing Citation Intent Wide And Deep Transformers Applied to Semantic Relatedness and Textual Entailment","link":"/2021/11/02/wide-deep-bert/"},{"title":"Lean about Huggingface Transformers","text":"","link":"/2021/02/27/transformers/"},{"title":"Model Compression","text":"Methods: Network Pruning Knowledge Distillation Parameter Quantization Architecture Design Reference: http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html https://www.bilibili.com/video/BV1yy4y1B7ny/ https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/Model-Compression/"},{"title":"Linux 命令","text":"删除行尾^Mvim下输入:%s/\\r//g 注：出现^M一般出现在本地文件上传至linux后，本地文件如excel的换行符默认为\\r\\n，而linux上的文件是\\n，因此会出现这个错误，直接替换即可。 替换字符串例：file.txt文件，cat替换成dog 文件内 :g/cat/s//dog/g 文件外 sed -i “s/cat/dog/g” file.txt 修改文件换行符 sed -i “s/\\r/\\n/g” file.txt 合并文件每行合并前提：文件行数相同方法一：$ paste -d “\\t” file_1 file_2 file_3 … &gt; new_file 方法二：$ awk ‘NR==FNR{a[NR]=$0;nr=NR;}NR&gt;FNR{print a[NR-nr]”\\t”$0}’ file_1 file_2 方法三：$ awk ‘{getline f2 &lt; “file_2”; print $0″\\t”f2}’ file_1 前后合并cat file1 file2 &gt; file3 显示文件某几行catcat filename | tail -n +3000 | head -n 1000 cat filename| head -n 3000 | tail -n +1000 分解：tail -n 1000：显示末尾1000行tail -n +1000：显示1000行以后的所有行head -n 1000：显示开头1000行 segsed -n ‘5,10p’ filename：显示5~10行 查找进程ps aux | grep “python” | awk -F ‘ ‘ ‘{print $2}’输出的即为正在运行python进程的PID 统计文件个数统计当期那文件夹下文件个数，包括子文件夹 1ls -lR | grep “^-“ | wc -l 统计文件夹下目录个数，包括子文件夹 1ls -lR | grep “^d” | wc -l 统计当前文件夹下文件个数 1ls -l | grep “^-“ | wc -l 统计当前文件夹下目录个数 1ls -l | grep “^d” | wc -l 中文乱码处理SecureCRT连接远程服务器后，发现有中文乱码问题，下面是解决方方案。 查看文件编码,打开文件，输入 1:set fileencoding 结果显示fileencoding=cp936 修改文件编码，输入 1:set fileencoding=utf-8 再次输入1中的语句检查文件编码，显示fileencoding=utf-8 编辑vimrc，在终端输入 1vim ~/.vimrc 在最后一行添加 1set encoding=utf-8 fileencodings=ucs-bom,utf-8,cp936 进程后台运行进程时间太久，可以通过ctrl+z让当前进程转至后台运行，此时，系统提示 1[1]+ Stopped 程序名 把进程转到后台运行 1bg 1 查看进程，在终端输入jobs，系统提示 1[1]+ Running 程序名 下载大文件在linux上下载uget和aria2 12sudo apt-get install ugetsudo apt-get install aria2 分享百度云盘文件，点击下载，暂停，查看下载地址，如：https://www.baidupcs.com/rest/2.0/pcs/file?method=batchdownload&amp;app_id=250528&amp;zipcontent=%7B%22fs_id%22%3A%5B%22206844088160893%22%5D%7D&amp;sign=DCb740ccc5511e5e8fedcff06b081203:4uhuulJJkDD%2FovWIkyeJonYrjFw%3D&amp;uid=3844112066&amp;time=1516433452&amp;dp-logid=432463554070908057&amp;dp-callid=0&amp;vuk=3039518624&amp;from_uk=3844112066 在linux上输入命令 1aria2c –dir=保存的目录 –max-connection-per-server=16 –max-concurrent-downloads=16 –split=16 –continue=true 下载连接地址 然后shell上就显示下载进度，如 1[#e55ab 12MiB/3.1Gib(%) CN:1 DL:61Kib ETA:16h48m39s] 至此，等待结束即可。 vim高亮查找在vim下输入/xxx，显示第一个匹配结果，回车，高亮显示所有匹配结果，按n向下查找 在vim下输入?xxx，显示最后一个匹配结果，回车，高亮所有匹配结果，按n向上查找 当回车并没有高亮所有匹配结果时，输入 1:set hls 不需要高亮所有时可关闭，输入 1:set nohls 添加用户并赋予root权限 在shell用root账户输入如下命令1sudo useradd -d /home/test -s /bin/bash -m test 其中， -d表示会在主目录生成test用户文件夹，并在该文件夹下生成环境文件（.bash_logout, .bashrc, .cache/, .distlib/, examples.desktop, .pip/, .profile）-s 表示shell 文件，一般都是/bin/bash-m 用户名 给test用户添加密码，这里要求输入一次密码和一次验证密码 1sudo passwd test 退出，用test用户登录，验证当前帐号： 查看 /etc/passwd，每个帐号一行，每行共7列：用户名，密码（x），uid，gid，用户信息说明，主文件夹，shell 查看 /etc/shadow, 用户密码（已加密） 查看/home/test,用ls -a查看.bashrc等隐藏文件是否存在 至此，添加用户与root权限完成。 gcc编译显示大量warning &amp; tensorflow安装失败的解决现象：在重新添加帐号后，安装librosa时，gcc编译过程中会出现大量的warning。并且，tensorflow始终安装失败，显示 12345Downloading/unpacking tensorflowCould not find any downloads that satisfy the requirement tensorflowCleaning up…No distributions at all found fo tensorflowStoring debug log for failure in /home/xxx/.pip/pip.log 原因：pip版本较低解决：升级pip，在shell输入命令 pip install –upgrade pip Reference linux-command https://www.cnblogs.com/zeze/p/6839230.html http://blog.chinaunix.net/uid-20732478-id-763411.html https://www.cnblogs.com/kex1n/p/5199109.html http://blog.csdn.net/lhw413/article/details/72806308 https://stackoverflow.com/questions/41875915/install-tensorflow-on-ubuntu-14-04","link":"/2016/11/02/linux-commands/"},{"title":"NLTK - Overview","text":"Language processing tasks and corresponding NLTK modules with examples of functionality Language processing task NLTK modules Functionality Accessing corpora corpus standardized interfaces to corpora and lexicons String processing tokenize, stem tokenizers, sentence tokenizers, stemmers Collocation discovery collocations t-test, chi-squared, point-wise mutual information Part-of-speech tagging tag n-gram, backoff, Brill, HMM, TnT Machine learning classify, cluster, tbl decision tree, maximum entropy, naive Bayes, EM, k-means Chunking chunk regular expression, n-gram, named-entity Parsing parse, ccg chart, feature-based, unification, probabilistic, dependency Semantic interpretation sem, inference lambda calculus, first-order logic, model checking Evaluation metrics metrics precision, recall, agreement coefficients Probability and estimation probability frequency distributions, smoothed probability distributions Applications app, chat graphical concordancer, parsers, WordNet browser, chatbots Linguistic fieldwork toolbox manipulate data in SIL Toolbox format Referencehttps://www.nltk.org/book/ch00.html#tab-goals","link":"/2021/11/03/nltk-overview/"},{"title":"NLTK - How to Onboard?","text":"Suggested course plans; approximate number of lectures per chapter Chapter Arts and Humanities Science and Engineering 1 Language Processing and Python 2-4 2 2 Accessing Text Corpora and Lexical Resources 2-4 2 3 Processing Raw Text 2-4 2 4 Writing Structured Programs 2-4 1-2 5 Categorizing and Tagging Words 2-4 2-4 6 Learning to Classify Text 0-2 2-4 7 Extracting Information from Text 2 2-4 8 Analyzing Sentence Structure 2-4 2-4 9 Building Feature Based Grammars 2-4 1-4 10 Analyzing the Meaning of Sentences 1-2 1-4 11 Managing Linguistic Data 1-2 1-4 Total 18-36 18-36 Referencehttps://www.nltk.org/book/ch00.html#tab-goals","link":"/2021/11/03/how-to-onboard-NLTK/"},{"title":"Epoch vs. Step","text":"Question: 1000 steps with 1 epoch vs running 100 steps with 10 epoch?Epoch: Number epoch equal to the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, one epoch has completed.Step: process one batch of data.Relation: An epoch consists of one full cycle through the training data. This are usually many steps. As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of 2,000 images / (10 images / step) = 200 steps. More: batch size, iteration, online learningIteration: Every time you pass a batch of data through the neural network, you completed one iteration. In the case of neural networks, that means the forward pass and backward pass.Relation: batch size * number of iterations = epoch Iteration: one batch of training samples.Online learning: batch_size=1 Batch size impact to modelToo small: it might get too slow because of significantly lower computational speed because of not exploiting vectorization to the full extent.Too large: out of memory. Reference: https://androidkt.com/batch-size-step-iteration-epoch-neural-network/ https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow#:~:text=In%20easy%20words.%20Epoch%3A%20Epoch%20is%20considered%20as,and%20batch_size%20%3D%201000%20steps%20%3D%20100.%20Share.","link":"/2021/11/11/epoch-vs-step/"},{"title":"Data Augmentation (DA)","text":"Data AugmentationThree DA methods: Paraphrasing, Noising, Sampling. ParaphrasingSix methods: Thesaurus, Semantic Embeddings, MLMs, Rules, Machine Translation, Model Generation. NoisingFive methods: Swapping, Deletion, Insertion, Substitution, Mixup.Others in ConSERT: Dropout, Feature Cut-off. SamplingFour methods: Rules, Seq2Seq Models, Language Models, Self-training. How to select DA methodsComparison in six dimensions.Level: t=text, e=embedding, l=labelGranularity: w=word, p=phrase, s=sentence. Tools for DA Easy DA: https://github.com/jasonwei20/eda_nlp Unsupervised DA：https://github.com/google-research/uda https://github.com/makcedward/nlpaug https://github.com/zhanlaoban/eda_nlp_for_Chinese Optimization for DA dataTwo methods: pre-train &amp; parameter sweeping. pre-trainPre-train with augmented data while fine-tune with labeled data. parameter sweeping DA Application Reference https://zhuanlan.zhihu.com/p/420295576 Data Augmentation Approaches in Natural Language Processing: A Survey","link":"/2021/12/03/data-augmentation/"},{"title":"坝上草原游记","text":"坝上草原游记这是我来帝都的第二个国庆节，这次的节日显得有点孤单，可能是因为我已经一个人出来租房子了吧。前23年的我一直都是和家人、同学一起度过，突然间变成孤身一个人了还真是有点不适应。幸好在偌大的帝都还有表姐，索性就和表姐约上，去北京周边走一走，看一看繁华边上的“京北第一草原”。 早上7:00在中服大厦门口坐上大巴，到目的地的时候已经是14:00左右了，离原定的到达时间晚了2h。不过一路上的风景还是不错的，平日里随处可见高楼大厦，偶尔来到偏远的地方，看看连绵的群山和崎岖的小路，阴暗的天气还夹杂着些小雨，倒别有一番风味了。 一下车便直奔坝上的酒店，路上奔波了大约7h，桌上的菜也凉了，看着没什么食欲，最美味的当数馒头了，幸好自己带了咸菜，就着馒头吃还不错。 由于天气原因，原定当天下午的骑马、越野项目改到了第二天上午，而第二天的大汗行宫之行也提前了。来之前就查了天气，这里昼夜温差大，又正逢秋季风大，所以大衣和披肩是来此旅游必需的装备。在酒店里换好衣服，披上披肩 ，我们就乘着大巴驶向大汗行宫。大汗行宫距酒店大约30分钟的车程，属于自费项目，原价90¥/人，满10人可团购85¥/人。本着下一次来也不知是猴年马月的想法，我们还是买了票，领略一下蒙古民族的“马背文化”。 大汗行宫占地面积很广，一进门就是崇天门，由三座门组成，庄严肃穆。 门后是沙场，放眼望去全是马匹与骑兵，气势恢宏。蒙古部落居住的房子叫蒙古包，正中间的应该是正殿，下面由大车轮拉着，我打趣说这是古代的房车啊。 !蒙古包呈圆形，顶部由圆环组成，像皇帝出行时轿上的华盖。褐色的顶，金色的壁，红色的毯，整体呈现一种庄严的对称美。殿内是一派上朝的景象，中间高坐的成吉思汗，两边正襟危坐的大臣，好似在紧张地商议某个战场。 往里走是鹰帐，帐前是一个庞大的鹰头，双目凌厉，目视前方，嘴唇紧闭，好似在思索，随时准备释放两边的爪子，发起攻击。 大汗行宫就像是个博物馆，蒙古包里再现了部分生活情景、记载了真实的历史事实，如史官商议、波斯美女跳舞，也有陈列历史文物、普及历史知识策，如战马的由来。逛了约1h，冷风吹得头疼，披肩此时就排上用场了，裹在头上刚好能抵挡些阴冷的风。 16:50集合，依旧乘坐大巴去闪电湖，这里的天黑的早，再加上昼夜温差大，部分人选择在车里等，不去闪电湖，而去了的每个人在回来时都不忘对天气埋怨一下。 晚餐有3种选择：自助BBQ、烤全羊、酒店晚餐，每个人的团费不一样，包含的游乐项目与晚餐种类也就不同，而我们的就包含了自助BBQ。自助BBQ和烤全羊在同一个地方，离酒店不远，由铁线网围着，只留一个进出口。拿着导游给的餐票去小屋里领取食材，然后拿火炉去取碳，一个简易的自助BBQ就开始了。 场所是露天的，铁线网外就是一个篝火活动广场，中间一群人围着篝火嗨皮，而广场前则是一个舞台，不时会有个主持人唱歌或调节氛围，也会有游客自告奋勇点歌上去唱。大家都很开心，旁边的老爷子一边烧烤一边还跳起了舞，感觉逃离了城市的喧嚣后，大家都放开了身心，不再拘谨。 第一天的行程很快在歌声中结束了，令我惊喜的是即便没有空调，酒店房间里也不冷，而且还有热水是关键。第二天才是重头戏，是我向往已久的草原行，为了养精蓄锐，我们草草地洗了个澡，回味了会儿白天的图片后，没多久就满意地入睡了。","link":"/2016/11/02/travel-in-bashangcaoyuan/"},{"title":"读书笔记——《大叔据智能》第2章：知识图谱","text":"读书笔记——《大叔据智能》第2章：知识图谱知识图谱的基本组成单元是实体（entity）；数据来源： 大规模知识库：Wikipedia、百度百科、Wordnet，实体关系、领域知识以结构化方式存储； 链接数据：采用RDF框架描述结构化知识，实体关系以三元组方式存在，RDF链接构成语义Web知识库； 网页文本数据：从无结构化网页中抽取结构化信息； 多数据源知识融合：实体、关系与实例融合； 主要应用：QU、QA、DR（Document Representation） 采用技术： Entity Linking：实体链指，包含实体识别（EntityRecognition）和实体消歧（EntityDisambiguation）两个任务，涉及文本图像、社交媒体等领域； Relation Extraction：关系抽取，有bootstrapping、短语抽取、实体对关系分类等方法；实体对关系分类方法中，可以用DistantSupervision启发式标注训练语料； Knowledge Reasoning：知识推理，涉及PathRanking Algorithnm、PredicateLogical、Markov LogicNetWork等方法，要求依赖规则，可采用关联挖掘技术自动发现推理规则； Knowledge Representation：知识表示，低维向量表示，采用DistributedRepresentation方案和TransE模型；TransE模型：分布式向量表示，实体关系转化成三元组，实体head-&gt;实体tail的过程中，不断调整h、r、t以达到h+r=t的目的。","link":"/2016/11/20/reading-knowledge-graph/"},{"title":"实习多少有点感悟","text":"今天与经理的one-one、与指导人的交流分别在处事方式、思维方式上让我收获很多。 处事方式在BD工作，是以目标和结果为导向的。拿到一个任务，首先要问指导人三个问题： 我这个任务是干什么的 要用什么方法和技术手段 评估指标是什么，要达到什么目的 在与指导人的沟通中，一定要check两人的理解是否一致。指导人的表达能力、被指导人的理解能力会影响整个任务的进行。在明白了任务后，要问指导人“我的理解是……你看这样对不对。”通过这样的方式，来统一两人的标准，以免不必要的重复工作。 在确定了上述3个问题后，整个工作会变得很清晰，而自己也会为了这个目标而去发散思维，不用时刻请教指导人。 经理说的对，一个任务失败了，很大程度上最大的受害者是被指导人。指导人会有多个项目，一个失败了还有其他的，然而被指导人就只有这么一个任务，一个任务黄了很大程度上反应了工作能力与技术水平，而且也影响了信誉度与自信，因此对自己负责才是最重要的。 思维方式今天在check分类效果时，发现训练正例里出现的词在测试集上没有召回，百思不得其解。指导人让我做了如下处理：对训练正例的句子统计词频，从大到小排序，留下length&gt;2且tf&gt;3的词，再人工过滤一遍，得到一个过滤词集。然后凡是句中出现了这类词语的通通滤掉。 结果效果很明显，召回了很多重要的句子。对此我问了指导人，他说这全看经验与时间累积的结果。正例内容未召回，说明存在负例的干扰，这时我就要去查看数据的因素。或许过滤部分词会导致负例缺失，但是可以先抱着扩大召回的目的去尝试，然后不断的修复。分类其实就是这个道理，分类实质上就是分离平面不断移动的一个过程，如果数据特别杂乱无章，正负例交错复杂，就算再牛掰的特征、模型也很难拟合这个平面，即便是拟合了在测试集上试验的结果也会不理想。因此对数据的处理很重要。 在NLP、ML、DL领域，有这么一个定律：神秘上——数据&lt;特征&lt;模型，而效果上正好相反。从数据到模型，理解的越深，越能体现一个人的厉害。然而实际上，往往对数据的处理是效果提升最明显的。当然，在图像和语音领域，DL、DNN的效果十分显著，但是这些是建立在数据庞大的基础之上的，如今图像和语音水平上模型是优于特征的，由于其海量数据的存在，模型能够自动学习到复杂的特征，甚至是人所想不到的特征。然而语言是一个特殊的领域，人的经验和规则起着非常重要的作用，况且数据匮乏，标注数据需要大量的人力，因此无论是DL还是DNN都还打不到理想的效果。就拿昨天标注的数据来说，一整天全神贯注也才标了6000条左右的数据，可见数据处理的重要性。 当然，上面的过程是一种思维方式的体现，我一个小小的实习生还打不到这样的水平，这是日积月累的结果。我要做的就是眼观六路耳听八方，不断的学习积累与经验总结，希望自己每天都能有所收获。","link":"/2016/12/01/internship-feeling/"},{"title":"课程学习——斯坦福CS224N深度学习自然语言处理lecture2","text":"课程第二讲主要介绍向量表达、word2vec原理以及skip-gram求上下文概率方程的推导过程。 WordNet本节视频里简单提了一下wordnet，但为了加深理解，我认为有必要对wordnet进行深层剖析。WordNet是一种基于认知语言学的、具有上下位关系的英语语义词典，根据词义组成了一个覆盖范围宽广的词汇语义网。 WordNet里有两个重要的概念，一个是同义词集，另一个是同义词之间的关系。它们分别用两个CSV文件来保存：synsets.txt和hypernym.txt。 同义词集synsets.txt保存了一系列的同义词，每行由3列，每列用逗号分隔，形如：id,synset,gloss其中；id=编号，synset=同义词集（单词间用空格分隔），gloss=注解举例： 45,AND_circuit AND_gate,a circuit in a computer that fires only when all of its inputs fire 关系hypernyms.txt保存同义词集之间的关系，每行是由逗号分隔的id序列，形如：id1,id2,id3… 继承关系（hyperonymy, hyponymy, is-a） 这类描述的是通用词与具体词之间的关系，比如：通用词：{furniture, piece_of_furniture}具体词：{bed} {bunkbed} 部分-整体关系(part-whole) 这类描述的是部分与整体之间的包含关系，比如整体={seat}，部分={leg} 交叉词性关系（Cross-POS） 实际上，WordNet主要按照词性（POS）来获取词间关系，因此WordNet含有4个子网络，分别存储名词、动词、形容词和副词。Cross-POS连接的是词干相同、词性不同的词，如ovserve(verb)、observant(adjective)、observation(noun)。 SAP（shortest ancestral path，最短祖先路径）SAP是WordNet的一种数据结构，表示两个不同结点到共同祖先的最短距离，这可以用来计算两个词之间的距离相似度。如图所示，结点3和结点11的SAP=4： 现有两个名词A和B，则A与B之间的距离distance(A,B)计算方式为：step1: 若A、B都不是名词，distance=∞step2: distance = min(sap(v(A),w(B))),其中V(A)表示A的任意同义词集，w(B)表示B的任意同义词集。 ##OutcastOutcast就是找出在一组词中与其他最不相似的词，计算方法相对SAP比较简单暴力，就是计算某词语其他词的距离之和，最大的那个就是要求的解了。即： 1(di)^2 = (dist(Ai, A1))^2 + (dist(Ai, A2))^2 + … + (dist(Ai, An))^2 注意，wordnet工具提供了distance(),sap()和outcast()方法。 NLTKL与WordNetNLTK也有WordNet接口，详情参考《NLTK源码阅读——WordNet》 word2vec尽管WordNet能够一定程度上表达单词得的含义，但是具有如下不足： 新词的时效性跟不上 需要人工去搜集、创建 不考虑上下文信息 难以准确地定位两个词的相似度 word2vec是google于2013年提出的工具，通过词向量（word embedding）来更好地度量词与词之间的相似性。起初vector的做法one-hot向量法：对于一个句子，如”I am a student.”，每个单词的向量表示为： 1234I: [1,0,0,0]am: [0,1,0,0]a: [0,0,1,0]student: [0,0,0,1] 即单词的向量维度K=词典大小，这很容易造成维度灾难问题。对于这种情况，对应的解决办法是利用向量空间模型（Vector Space Model，简称VSM）将one-hot向量映射为稠密连续的Distributed Representation，其基本思想是：通过训练将每一个词映射成一个固定长度的短向量，构成词向量空间，每一个词向量相当于是空间中的一个点，那么点间距离就可以用来度量词间相似度了。 Distributed Representation的形式如[0.792,-0.177,-0.107,0.109,-0.542,…]，维度一般是50-100维，向量表示不唯一。 word2vec主要包含Skip-gram和Continious Bag of Words(CBOW)两种模型，分别采用Hierarchical Softmax 和Negative Sampling两种框架来实现。Ski-gram指给定中心词求上下文的概率，CBOW则相反，指给定上下文求中心词的概率。下面以Hierachical Softmax实现CBOW模型为例介绍实现的过程。 CBOW模型的Hierachical Softmax实现模型有3层：输入层、投影层、输出层. 注意：word embedding是术语表达，而word2vec是google提出的工具，并非指模型或算法。在word2vec中采用Huffman编码，利用词频作为Huffman树的结点权值，权值大的编码为1，为左孩子结点；权值小的编码为0，为右孩子结点。 SG（Skip-Gram）SG根据中心词预测上下文，包含三层：input（输入层）、projection（投影层）、output（输出层） CBOW（Continious Bag of Words）CBOW正好与SG相反，根据上下文预测中心词。 Hierachical Softmax Negative Sample Reference wordnet 《Word2Vec的前世今生》 《Distributed Representations ofWords and Phrases and their Compositionality》","link":"/2017/04/08/stanford-nlp-lecture/"},{"title":"常用工具安装","text":"office2016安装与激活 下载office2016,解压，安装 https://pan.baidu.com/s/1htsmyxQ 下载激活工具kms，解压，安装 https://pan.baidu.com/s/1kXafD1X 打开kms，点击红色按钮，激活成功自动关闭，打开office，即可使用。 SecureCRT安装与激活Windows 下载SecureCRT（含注册机） https://pan.baidu.com/s/1nwRUdjr 点击scrt8.13_x64.exe，安装完成后不要打开，使其处于未使用状态 将/8.x.x注册机/keygen.exe拷贝至步骤2中SecureCRT的安装目录下，运行keygen.exe 点击patch，中间会要求选择SecureCRT.exe和LicenceHelper.exe的位置，由于已经放在了SecureCRT安装目录下，会自动显示对应的文件，选择即可。 打开SecureCRT8.1, 点输入注册码-&gt;人工输入，将keygen里的name、company、date、licence number填入相应的地方即可，feature留白不用填。 至此，安装与激活结束。 Ubuntu和Windows类似，下载安装即可。https://pan.baidu.com/s/1rasmvzI","link":"/2018/01/27/tools/"},{"title":"anaconda——unbuntu16.04下安装","text":"在官网下载Anaconda，注意版本 https://www.anaconda.com/download/#linux 在终端输入如下命令12cd 下载bash Anaconda2-5.0.1-Linux-x86_64.sh 一直按回车键，中间有两次要填写yes的时候 第一次：同意接受协议 第二次：同意将Anaconda2安装路径写入/home/joanna/.bashrc 至此，安装成功，最后出现提示 Thank you for installing Anaconda2! 验证，在终端输入python，发现还是系统默认的pyhton，没有Anaconda等字样出现，则在shell输入命令1source ~/.bashrc 重新输入python，系统显示1python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19) 在shell输入命令1conda list 系统显示一大堆可用的packages，至此验证结束。 Reference http://blog.csdn.net/woainishifu/article/details/74978647","link":"/2018/01/20/ubuntu-install-anaconda/"},{"title":"课程学习——斯坦福CS224N深度学习自然语言处理lecture1","text":"最近斯坦福大学新推出了一门课《Natural Language Processing with Deep Learning》，这对于NLP领域的初学者来说是一门很好的教学课程。本人作为nlp小白，写此文主要作为个人的学习笔记，有什么不当之处还请读者指点。lecture1以入门介绍为主，讲述nlp、dl的定义、历史发展和基本方法。 NLP(Natural Language Processing)NLP是一门CS（计算机科学）、AI（人工智能）、Linguistic（语言学）交叉的学科，研究的是如何让计算机理解人类的自然语言的一门学科。若输入的是语音，则先进行语音分析（phonetic analysis）；若输入是文本，则先进行OCR文字识别或切词处理（Tokenization），然后进行如下四个层次的分析； 形态学分析（morphological analysis） 语法分析（syntacic analysis） 语义分析（semantic analysis） 语篇处理（discourse processing） Human Language如果把语言比作信号，那么人类语言就像是一个精确的信号系统，可以通过声音、动作和图像来进行编码，大脑通过连续不断地的激活以获得最准确的意思。此外，人类本身大脑存储了海量的知识库，表达的语言具有上下文强依赖关系。 DL(Deep Learning)DL属于ML（Machine Learning）的一个分支，解决的是机器真正自动学习的问题。在80~00年代，ML任务并非真正的机器学习任务，需要人为地设计表征形式和特征，而这个过程也是最重要、工作量最大的部分，整个过程需要人去艰难地学习理论知识、设计各种特征、分析预测结果……久而久之ML任务就变成了一个参数优化的体力活。 DL有个比较重要的概念——表征学习（Representation Learning），这是一个自动学习的过程，能够从原始的信号中自动学习到好的特征，总结出优秀、重要的表征形式。近年来随着GPU、分布式等存储计算能力的提高、数据集的扩大，DL在语音识别和图像处理两个领域的突破尤为显著。DL和ML的对比可以从图中看出： Deep NLP从近来的研究论文中也能发现，深度学习开始在NLP领域占领重要的作用。不像原来的手工提取特征计算的方式，Deep NLP有个重要的概念——向量空间（vector spaces），首先将文字转化成向量，然后向量结合成新的向量，用激活函数得到新的表征。例如： 传统的NLP形态学分析，单词由前缀（prefix）+词干（stem）+后缀（suffix）组成，而DL将其转化成了vector，两两带权重累加形成新的向量，如图所示： 机器翻译是NLP研究的开端，SMT（Statistical Machine Learning）是NLP发展史一个的里程碑，而如今的机器翻译大多以NMT（Nerual Machine Learning）为主，将源语句子映射为一个向量后再处理输出目标语句子。","link":"/2017/04/15/stanford-nlp-lecture1/"},{"title":"北京3日游","text":"2018.1.21闺蜜11:30在首都机场降落，行了近1.5h的路程终于来到了学校。 湖面早已结了厚厚的冰，学校开放了一半的区域供给校内外人员溜冰玩耍，我们当然不会错过这个机会。 冰面很厚实，但也有些危险，不经意间还会踩到一条条小沟，不怎么会滑的人稍不注意可能会跌倒。我们两个走走停停，有时木讷地立在原地，好久都不敢迈出一步。 我们常常驻足原地，观察身边来来去去的同学们。原来，只滑过一两次的也不只有我们，许多人像我们一样，每踏出一步都是颤颤兢兢的模样，我们心底竟有些窃喜。 我们似乎是在数着时间度过一般，两个小时就好像一天那么久。我们自己的鞋子在边上被风刮、被冰打，脚轻轻一碰就好象伸进了冰窖，不住地打寒颤。 风呼呼地吹，晚上我们来到小西门外的小吊梨汤，浓浓地琵琶梨汤灌下肚里温暖了整个胃，好不惬意。 2018.1.22上我们两个从不是那种先完美地规划，然后严格按照计划来执行的人，尤其是旅游这件事。 今天的行程是上午798艺术区，下午奥森公园。这两个地方，单是任何一个想要游览尽兴，不下3、4个小时是完不成的。而我们这两只从宿舍出门，拐到食堂吃完包子便已经10点了，如此散漫，也是好笑。 路经坎坷（下错站、坐过站、走错路），花了1个多小时，我们总算来到了798艺术区。不得不说，这些废旧厂房改造得很成功，仿照德国包豪斯风格，四周浓浓的艺术氛围扑面而来，我们这两只伪文艺都忍不住赞叹。 这里艺术画廊和展览区随处可见，种类也各异。有悬浮的家具， 雕塑的天使，琳琅的波斯绸缎，美论美奂。 在画廊里，我们随意走走看看，感叹画作的美妙，却并不懂画的真谛。被这些画所吸引，我们毫不犹豫地买下了几张卡片，拿回去当装饰用。 由于下午还有行程，时间仓促，我们在大众点评上找了家适合学生党的餐厅，吃完后便匆匆前往奥森公园。 2018.1.22下一出地铁站，我们就感受到了刺骨的寒冷。大寒刚过，这两天北京降温明显，冷风刮在脸上就像冰刀一样，空旷的平地上只听见我们的嗷嗷惨叫，好在空气很不错。冰面上也已经结成了冰，但似乎并没有多后，能够清晰地看到冰与水的界限，三两只鸭子在扑腾水面，冰意袭来，我们忍不住对这几只鸭子心疼起来。 从儿童游乐园右方走进去，便是一条紧临湖边的长长小道，枯枝高耸，人烟稀少,除了出来健身的附近居民，森林公园里只有稀稀落落的几个游客，在枯草秃木的陪衬下整个公园尤其显得萧瑟。 没有管理员，我们悄悄地走近湖边，看到了一个摄像机装备，两个大叔安静地站在那儿，似乎在等待最美的一瞬。从他们身边走过，我们惊喜地发现被落叶遮盖的冰面，小心翼翼地踩在冰面上，生怕一个不小心这冰就裂了。 从奥运塔望过去，森林公园的湖望不到尽头。起初我们只想随意看看，可不知不觉间竟把整个湖给走了一圈。 黄昏过后，我们在奥运购物中心的云海肴解决了晚饭，出来的时候还发生了一个小插曲。两小只根据路牌找洗手间，走了一大圈，发现竟是走回了云海肴的门口，抬头那一瞬间快笑岔了气。 我们赶到鸟巢门口的时候已经是8点多了，夜幕下，鸟巢连着那水立方在黄蓝灯光的映衬下显得有点梦幻。 闺蜜哆嗦地拿起手机，想要记录下美景，而我将这一景一人永远地定格了下来。 2018.1.23最后一天只有半天的行程，我们便逛了下最近的圆明园。都说圆明园是国人心中永远的痛，这儿原本就萧条，严冬更是给这些殘垣断壁添加了浓重的荒凉之色。坐在一片空地上，谁能想到三百年前这儿原有多么的繁荣热闹，想到这儿，我们更加落寞了。 幸运的是，在这个毫无生气的园内，我们看到了欢快戏水的黑天鹅，它们的存在给我们游人带来了些许安慰。 最后时间有限，我们最后直接奔着全景模型和西洋楼遗址去了。站在西洋楼遗址旁，我总能想起还珠格格和如懿传，不禁失笑，果然电视剧和杂书还是看太多了。","link":"/2018/01/23/travel-in-beijing/"},{"title":"Ensemble (1)","text":"WhatEnsemble learning is a combination of multiple machine learning techniques performed together. Ensemble learning techniques can be further divided under a broad umbrella of three classes: mixing training data, mixing combinations, and mixing models. Mixing training data: divide the training data into multiple chunks, and train separate classifiers on each subset of training data. Benifit: Ensures that classifiers capture sufficient diversity as they are trained (evolve) on a subset of the population. Achieve superior accuracy compared to a case in which we trained a single learner on a whole population (training data). Mixing CombinationsTwo mixing ways:boosting and stacking. Boosting: If the model learner has a weak performance, we could give more emphasis to that particular learner with increased emphasis where previous learner had weak performance. Stacking: place one machine learning model on top of the output of another to do stacks of machine learning models. We treat the result of individual predictions as the next training data. Two layers models: 1st layer - base learners, 2nd layer - meta learner. Mixing Models Reference https://www.oreilly.com/library/view/ensemble-learning-for/9781484259405/ https://www.oreilly.com/library/view/hands-on-ensemble-learning/9781789612851/ https://www.oreilly.com/library/view/ensemble-machine-learning/9781789136609/","link":"/2021/12/18/Ensemble/"},{"title":"About Me","text":"Hi, I am Feifei. Happy to see you here. #^o^# Any question, please contact me via my github website.","link":"/2015/09/01/about/"},{"title":"Ensemble (2) - Stacking","text":"Note: image is downloaded from here. WhatStacking is one of mixing comianition methods in ensemble learning. It’s a total different way to boosting. Two layers models in stacking: L1: Base learners. To train multiple models to get predictions. L2: Meta learner. To treat base learners predictions as next training data. Code (Scikit-Learn)https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html Stacking Classification1234567891011121314151617from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.svm import LinearSVCfrom sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import make_pipelinefrom sklearn.ensemble import StackingClassifierX, y = load_iris(return_X_y=True)estimators = [ (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)), (\"svr\", make_pipeline(StandardScaler(), LinearSVC(random_state=42))),]clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)clf.fit(X_train, y_train).score(X_test, y_test)# Output: 0.9… Sstacking Regression123456789101112131415from sklearn.datasets import load_diabetesfrom sklearn.linear_model import RidgeCVfrom sklearn.svm import LinearSVRfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.ensemble import StackingRegressorX, y = load_diabetes(return_X_y=True)estimators = [(\"lr\", RidgeCV()), (\"svr\", LinearSVR(random_state=42))]reg = StackingRegressor( estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=10, random_state=42),)from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)reg.fit(X_train, y_train).score(X_test, y_test)# Output: 0.3... Reference https://learning.oreilly.com/library/view/ensemble-learning-for/9781484259405/html/489264_1_En_4_Chapter.xhtml","link":"/2021/12/19/Ensemble-Stacking/"},{"title":"Ensemble (3) - Outlier Detection","text":"Note: image is downloaded from here. WhatReference:https://learning.oreilly.com/library/view/ensemble-learning-for/9781484259405/html/489264_1_En_6_Chapter.xhtml","link":"/2021/12/19/Ensemble-Outlier-Detection/"},{"title":"Data Quality (2)","text":"","link":"/2021/12/21/data-quality-2/"},{"title":"Variance or Bias?","text":"High VarianceHigh variance means the model is too complicated for the amount of data we have. It leads to overfitting. Situation of high variance: training error is much lower than the testing error. How to address high variance problem? reduce the number of features increase the number of data points High BiasHigh bias means the model ist too simple to explain the data we have. In such scenario, adding more data won’t help. How to addresss high bias? add more features","link":"/2021/12/22/variancd-or-bias/"},{"title":"チルドレンレコード","text":"(function(){ function loadcplayer() { if (typeof window.cplayerList === 'undefined') window.cplayerList = {}; if (typeof window.cplayerList[\"cplayer-79286479\"] !== 'undefined') return; if (!cplayer.prototype.add163) cplayer.prototype.add163 = function add163(id) { if (!id) throw new Error(\"Unable Property.\"); return fetch(\"https://music.huaji8.top/?id=\" + id).then(function(res){return res.json()}).then(function(data){ let obj = { name: data.info.songs[0].name, artist: data.info.songs[0].ar.map(function(ar){ return ar.name }).join(','), poster: data.pic.url, lyric: data.lyric.lyric, sublyric: data.lyric.tlyric, src: data.url.url } this.add(obj); return obj; }.bind(this)) } window.cplayerList[\"cplayer-79286479\"] = new cplayer({ element: document.getElementById(\"cplayer-79286479\"), playlist: [{\"name\":\"チルドレンレコード\",\"artist\":\"96猫,伊東歌詞太郎\",\"poster\":\"https://cplayer.js.org/801422833716a4f0f96ff6dff1f77dfe.jpg\",\"src\":\"https://cplayer.js.org/8af423669c27d265bb129d04a927044f.mp3\"}], generateBeforeElement: true, deleteElementAfterGenerate: true, autoplay: true }); window.cplayerList[\"cplayer-79286479\"].add163(27955597) } if (typeof window.cplayer === 'undefined' && !document.getElementById(\"cplayer-script\")) { var js = document.createElement(\"script\"); js.src = 'https://cdn.jsdelivr.net/gh/MoePlayer/cPlayer/dist/cplayer.js'; js.id = \"cplayer-script\"; js.addEventListener(\"load\", loadcplayer); document.body.appendChild(js); } else { window.addEventListener(\"load\", loadcplayer); } })()","link":"/2021/12/23/music-test/"},{"title":"归途","text":"(function(){ function loadcplayer() { if (typeof window.cplayerList === 'undefined') window.cplayerList = {}; if (typeof window.cplayerList[\"cplayer-30206162\"] !== 'undefined') return; if (!cplayer.prototype.add163) cplayer.prototype.add163 = function add163(id) { if (!id) throw new Error(\"Unable Property.\"); return fetch(\"https://music.huaji8.top/?id=\" + id).then(function(res){return res.json()}).then(function(data){ let obj = { name: data.info.songs[0].name, artist: data.info.songs[0].ar.map(function(ar){ return ar.name }).join(','), poster: data.pic.url, lyric: data.lyric.lyric, sublyric: data.lyric.tlyric, src: data.url.url } this.add(obj); return obj; }.bind(this)) } window.cplayerList[\"cplayer-30206162\"] = new cplayer({ element: document.getElementById(\"cplayer-30206162\"), playlist: [{\"name\":\"归途\",\"artist\":\"薛晓\",\"poster\":\"https://cplayer.js.org/801422833716a4f0f96ff6dff1f77dfe.jpg\",\"src\":\"https://music.163.com/#/song?id=427606738\"}], generateBeforeElement: true, deleteElementAfterGenerate: true, autoplay: true }); window.cplayerList[\"cplayer-30206162\"].add163(427606738) } if (typeof window.cplayer === 'undefined' && !document.getElementById(\"cplayer-script\")) { var js = document.createElement(\"script\"); js.src = 'https://cdn.jsdelivr.net/gh/MoePlayer/cPlayer/dist/cplayer.js'; js.id = \"cplayer-script\"; js.addEventListener(\"load\", loadcplayer); document.body.appendChild(js); } else { window.addEventListener(\"load\", loadcplayer); } })()","link":"/2021/12/24/guitu/"}],"tags":[{"name":"diary","slug":"diary","link":"/tags/diary/"},{"name":"NLP,Transformer","slug":"NLP-Transformer","link":"/tags/NLP-Transformer/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"travel","slug":"travel","link":"/tags/travel/"},{"name":"knowledge graph","slug":"knowledge-graph","link":"/tags/knowledge-graph/"},{"name":"internship","slug":"internship","link":"/tags/internship/"},{"name":"software","slug":"software","link":"/tags/software/"},{"name":"anaconda","slug":"anaconda","link":"/tags/anaconda/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"machine learning, ensemble","slug":"machine-learning-ensemble","link":"/tags/machine-learning-ensemble/"},{"name":"machine learning, data analysis","slug":"machine-learning-data-analysis","link":"/tags/machine-learning-data-analysis/"}],"categories":[{"name":"Music","slug":"Music","link":"/categories/Music/"},{"name":"Technology","slug":"Technology","link":"/categories/Technology/"},{"name":"LifeStyle","slug":"LifeStyle","link":"/categories/LifeStyle/"},{"name":"technology","slug":"technology","link":"/categories/technology/"},{"name":"About","slug":"About","link":"/categories/About/"}]}