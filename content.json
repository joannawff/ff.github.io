{"pages":[],"posts":[{"title":"Sat May 22 2021 08:00:00 GMT+0800 (中国标准时间)","text":"W: What if I die before you?X: Continue my life.W: (Silence)X: Disappointed?W: Yeah.X: I need give our parents the end of their care.W: (Silence)X: I would try to die before you.W: I’m afraid nobody would take care of our parents then.(Silence for a while)X: If you’ve got cancer, let’s resign and go traverling around the world with all of our belongings.W: (Crying) OK.","link":"/2021/05/22/2021-05-22/"},{"title":"BERT and its family","text":"Reference: https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/BERT-and-its-family/"},{"title":"Roberta","text":"","link":"/2021/05/19/Roberta/"},{"title":"Word2Vec vs BPE vs WordPiece","text":"Reference: https://wmathor.com/index.php/archives/1517/","link":"/2021/05/19/BPE-vs-WordPiece/"},{"title":"Open Corpora","text":"https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words","link":"/2021/05/25/Open-Corpora/"},{"title":"data-quality.md","text":"","link":"/2021/03/11/data-quality-md/"},{"title":"Data quality or model quality?","text":"[https://www.talend.com/resources/machine-learning-data-quality/](https://www.talend.com/resources/machine-learning-data-quality/","link":"/2021/03/11/data-quality/"},{"title":"Get Start with Hexo","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/02/27/hello-world/"},{"title":"Multilingual Pretrained Models","text":"","link":"/2021/02/27/multilingual-pretrained-models/"},{"title":"language detect","text":"resource: https://pypi.org/project/langdetect/","link":"/2021/05/25/language-detect/"},{"title":"photo","text":"","link":"/2021/03/11/photo/"},{"title":"wide&deep bert","text":"How to integrate traditional models like GBDT/LR to popular transformer based pretrain-finetune models like BERT/Roberta? Reference: Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm GBDT and BERT: a Hybrid Solution for Recognizing Citation Intent Wide And Deep Transformers Applied to Semantic Relatedness and Textual Entailment","link":"/2021/11/02/wide-deep-bert/"},{"title":"Lean about Huggingface Transformers","text":"","link":"/2021/02/27/transformers/"},{"title":"Model Compression","text":"Methods: Network Pruning Knowledge Distillation Parameter Quantization Architecture Design Reference: http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html https://www.bilibili.com/video/BV1yy4y1B7ny/ https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/Model-Compression/"},{"title":"Linux Commands","text":"Please click linux-command to view more commands.","link":"/2021/03/03/linux-commands/"}],"tags":[{"name":"diary","slug":"diary","link":"/tags/diary/"},{"name":"NLP,Transformer","slug":"NLP-Transformer","link":"/tags/NLP-Transformer/"},{"name":"NLP, ML, DL, data analysis","slug":"NLP-ML-DL-data-analysis","link":"/tags/NLP-ML-DL-data-analysis/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"linux","slug":"linux","link":"/tags/linux/"}],"categories":[]}