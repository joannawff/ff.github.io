{"pages":[],"posts":[{"title":"My Love","text":"What if I die before you? Continue my life. (Silence) Disappointed? Yeah. I need give our parents the end of their care. (Silence) I would try to die before you. I’m afraid nobody would take care of our parents then. (Silence) If you’ve got cancer, let’s resign and go traverling around the world with all of our belongings. OK.","link":"/2021/05/22/2021-05-22/"},{"title":"BERT and its family","text":"Reference: https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/BERT-and-its-family/"},{"title":"Roberta","text":"","link":"/2021/05/19/Roberta/"},{"title":"Word2Vec vs BPE vs WordPiece","text":"Reference: https://wmathor.com/index.php/archives/1517/","link":"/2021/05/19/BPE-vs-WordPiece/"},{"title":"Open Corpora","text":"https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words","link":"/2021/05/25/Open-Corpora/"},{"title":"data-quality.md","text":"","link":"/2021/03/11/data-quality-md/"},{"title":"Data quality or model quality?","text":"[https://www.talend.com/resources/machine-learning-data-quality/](https://www.talend.com/resources/machine-learning-data-quality/","link":"/2021/03/11/data-quality/"},{"title":"Get Start with Hexo","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/02/27/hello-world/"},{"title":"Multilingual Pretrained Models","text":"","link":"/2021/02/27/multilingual-pretrained-models/"},{"title":"language detect","text":"resource: https://pypi.org/project/langdetect/","link":"/2021/05/25/language-detect/"},{"title":"photo","text":"","link":"/2021/03/11/photo/"},{"title":"wide&deep bert","text":"How to integrate traditional models like GBDT/LR to popular transformer based pretrain-finetune models like BERT/Roberta? Please Reference: Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm GBDT and BERT: a Hybrid Solution for Recognizing Citation Intent Wide And Deep Transformers Applied to Semantic Relatedness and Textual Entailment","link":"/2021/11/02/wide-deep-bert/"},{"title":"Lean about Huggingface Transformers","text":"","link":"/2021/02/27/transformers/"},{"title":"Model Compression","text":"Methods: Network Pruning Knowledge Distillation Parameter Quantization Architecture Design Reference: http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html https://www.bilibili.com/video/BV1yy4y1B7ny/ https://wmathor.com/index.php/archives/1508/","link":"/2021/05/19/Model-Compression/"},{"title":"Linux Commands","text":"Please click linux-command to view more commands.","link":"/2021/03/03/linux-commands/"},{"title":"What does NLTK do?","text":"Language processing tasks and corresponding NLTK modules with examples of functionality Language processing task NLTK modules Functionality Accessing corpora corpus standardized interfaces to corpora and lexicons String processing tokenize, stem tokenizers, sentence tokenizers, stemmers Collocation discovery collocations t-test, chi-squared, point-wise mutual information Part-of-speech tagging tag n-gram, backoff, Brill, HMM, TnT Machine learning classify, cluster, tbl decision tree, maximum entropy, naive Bayes, EM, k-means Chunking chunk regular expression, n-gram, named-entity Parsing parse, ccg chart, feature-based, unification, probabilistic, dependency Semantic interpretation sem, inference lambda calculus, first-order logic, model checking Evaluation metrics metrics precision, recall, agreement coefficients Probability and estimation probability frequency distributions, smoothed probability distributions Applications app, chat graphical concordancer, parsers, WordNet browser, chatbots Linguistic fieldwork toolbox manipulate data in SIL Toolbox format Referencehttps://www.nltk.org/book/ch00.html#tab-goals","link":"/2021/11/03/nltk-overview/"},{"title":"How to Onboard NLTK?","text":"Suggested course plans; approximate number of lectures per chapter Chapter Arts and Humanities Science and Engineering 1 Language Processing and Python 2-4 2 2 Accessing Text Corpora and Lexical Resources 2-4 2 3 Processing Raw Text 2-4 2 4 Writing Structured Programs 2-4 1-2 5 Categorizing and Tagging Words 2-4 2-4 6 Learning to Classify Text 0-2 2-4 7 Extracting Information from Text 2 2-4 8 Analyzing Sentence Structure 2-4 2-4 9 Building Feature Based Grammars 2-4 1-4 10 Analyzing the Meaning of Sentences 1-2 1-4 11 Managing Linguistic Data 1-2 1-4 Total 18-36 18-36 Referencehttps://www.nltk.org/book/ch00.html#tab-goals","link":"/2021/11/03/how-to-onboard-NLTK/"},{"title":"Epoch vs. Step","text":"Question: 1000 steps with 1 epoch vs running 100 steps with 10 epoch?Epoch: Number epoch equal to the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the dataset, one epoch has completed.Step: process one batch of data.Relation: An epoch consists of one full cycle through the training data. This are usually many steps. As an example, if you have 2,000 images and use a batch size of 10 an epoch consists of 2,000 images / (10 images / step) = 200 steps. More: batch size, iteration, online learningIteration: Every time you pass a batch of data through the neural network, you completed one iteration. In the case of neural networks, that means the forward pass and backward pass.Relation: batch size * number of iterations = epoch Iteration: one batch of training samples.Online learning: batch_size=1 Batch size impact to modelToo small: it might get too slow because of significantly lower computational speed because of not exploiting vectorization to the full extent.Too large: out of memory. Reference: https://androidkt.com/batch-size-step-iteration-epoch-neural-network/ https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow#:~:text=In%20easy%20words.%20Epoch%3A%20Epoch%20is%20considered%20as,and%20batch_size%20%3D%201000%20steps%20%3D%20100.%20Share.","link":"/2021/11/11/epoch-vs-step/"},{"title":"measure data quality","text":"","link":"/2021/11/18/measure-data-quality/"}],"tags":[{"name":"diary","slug":"diary","link":"/tags/diary/"},{"name":"NLP,Transformer","slug":"NLP-Transformer","link":"/tags/NLP-Transformer/"},{"name":"NLP, ML, DL, data analysis","slug":"NLP-ML-DL-data-analysis","link":"/tags/NLP-ML-DL-data-analysis/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"linux","slug":"linux","link":"/tags/linux/"}],"categories":[]}